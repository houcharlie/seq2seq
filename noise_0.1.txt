/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.
  warnings.warn(
/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.
  warnings.warn(
/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.
  warnings.warn(
/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.
  warnings.warn(
/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.
  warnings.warn(
/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.
  warnings.warn(
/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.
  warnings.warn(
/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.
  warnings.warn(
06/22/2023 00:55:09 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 5
Local process index: 5
Device: cuda:5

Mixed precision type: no

06/22/2023 00:55:09 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: no

06/22/2023 00:55:10 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 7
Local process index: 7
Device: cuda:7

Mixed precision type: no

06/22/2023 00:55:10 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

06/22/2023 00:55:10 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: no

06/22/2023 00:55:10 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: no

06/22/2023 00:55:10 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 4
Local process index: 4
Device: cuda:4

Mixed precision type: no

06/22/2023 00:55:10 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 6
Local process index: 6
Device: cuda:6

Mixed precision type: no

06/22/2023 00:55:13 - WARNING - datasets.builder - Found cached dataset bookcorpus (/home/ubuntu/huggingface/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)
loading file vocab.json
loading file merges.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
loading configuration file /home/ubuntu/bartone/reconstruction/no_noise/config.json
Model config BartConfig {
  "_name_or_path": "facebook/bart-base",
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGenerationOne"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.1,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.27.4",
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file /home/ubuntu/bartone/reconstruction/no_noise/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

All model checkpoint weights were used when initializing BartForConditionalGenerationOneNoise.

All the weights of BartForConditionalGenerationOneNoise were initialized from the model checkpoint at /home/ubuntu/bartone/reconstruction/no_noise.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGenerationOneNoise for predictions without further training.
loading configuration file /home/ubuntu/bartone/reconstruction/no_noise/generation_config.json
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

Running bart tokenizers on train dataset (num_proc=51):   0%|          | 0/666037 [00:00<?, ? examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 0/666037 [00:00<?, ? examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 0/666037 [00:00<?, ? examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 0/666037 [00:00<?, ? examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 0/666037 [00:00<?, ? examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 0/666037 [00:00<?, ? examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 0/666037 [00:00<?, ? examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 1000/666037 [00:00<09:13, 1200.83 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 1000/666037 [00:00<10:14, 1082.41 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 1000/666037 [00:00<10:28, 1057.65 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 1000/666037 [00:00<10:37, 1043.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 2000/666037 [00:00<04:50, 2282.82 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|          | 4000/666037 [00:01<02:16, 4848.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|          | 6000/666037 [00:01<01:28, 7478.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|          | 5000/666037 [00:01<01:50, 6000.73 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|          | 5000/666037 [00:01<01:40, 6567.43 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|          | 8000/666037 [00:01<01:04, 10270.50 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 13000/666037 [00:01<00:38, 17031.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 11000/666037 [00:01<00:45, 14250.02 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 13000/666037 [00:01<00:37, 17376.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 11000/666037 [00:01<00:42, 15405.02 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 20000/666037 [00:01<00:25, 25669.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 16000/666037 [00:01<00:32, 20094.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 1000/666037 [00:01<13:49, 801.38 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 17000/666037 [00:01<00:33, 19650.15 examples/s]Running bart tokenizers on train dataset (num_proc=51):   4%|▍         | 25000/666037 [00:01<00:24, 26541.09 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 15000/666037 [00:01<00:38, 17061.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|          | 5000/666037 [00:01<02:21, 4656.89 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 20000/666037 [00:01<00:27, 23141.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 22000/666037 [00:01<00:27, 23023.28 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▍         | 30000/666037 [00:01<00:23, 27515.07 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 21000/666037 [00:01<00:35, 18322.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 1000/666037 [00:01<16:23, 676.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):   4%|▎         | 24000/666037 [00:01<00:25, 24982.19 examples/s]Running bart tokenizers on train dataset (num_proc=51):   4%|▍         | 26000/666037 [00:01<00:25, 24910.00 examples/s]Running bart tokenizers on train dataset (num_proc=51):   4%|▍         | 26000/666037 [00:01<00:27, 23321.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 2000/666037 [00:01<07:27, 1482.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▌         | 34000/666037 [00:01<00:24, 25875.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▍         | 33000/666037 [00:01<00:18, 34510.00 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 10000/666037 [00:01<01:20, 8141.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▍         | 33000/666037 [00:01<00:20, 31230.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):   4%|▍         | 28000/666037 [00:01<00:27, 23237.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):   6%|▌         | 40000/666037 [00:01<00:14, 42810.73 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|          | 6000/666037 [00:01<02:02, 5366.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):   6%|▌         | 38000/666037 [00:01<00:22, 27307.50 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 15000/666037 [00:01<00:50, 13005.94 examples/s]Running bart tokenizers on train dataset (num_proc=51):   6%|▌         | 40000/666037 [00:01<00:16, 38700.73 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▌         | 34000/666037 [00:01<00:20, 30716.85 examples/s]Running bart tokenizers on train dataset (num_proc=51):   7%|▋         | 48000/666037 [00:02<00:14, 42576.89 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|▏         | 9000/666037 [00:01<01:21, 8044.11 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 19000/666037 [00:01<00:41, 15725.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):   6%|▋         | 42000/666037 [00:02<00:15, 41272.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):   7%|▋         | 46000/666037 [00:02<00:16, 38088.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):   7%|▋         | 46000/666037 [00:02<00:17, 35296.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):   8%|▊         | 54000/666037 [00:02<00:14, 42863.30 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 12000/666037 [00:02<00:59, 10954.18 examples/s]Running bart tokenizers on train dataset (num_proc=51):   8%|▊         | 50000/666037 [00:02<00:12, 49723.27 examples/s]Running bart tokenizers on train dataset (num_proc=51):   8%|▊         | 56000/666037 [00:02<00:11, 51553.78 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 22000/666037 [00:02<00:38, 16730.52 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 1000/666037 [00:01<21:11, 523.14 examples/s]Running bart tokenizers on train dataset (num_proc=51):   8%|▊         | 55000/666037 [00:02<00:13, 45474.69 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 15000/666037 [00:02<00:45, 14156.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):   9%|▉         | 63000/666037 [00:02<00:12, 47847.82 examples/s]Running bart tokenizers on train dataset (num_proc=51):   4%|▍         | 26000/666037 [00:02<00:30, 20837.62 examples/s]Running bart tokenizers on train dataset (num_proc=51):   9%|▊         | 57000/666037 [00:02<00:11, 50932.35 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 3000/666037 [00:02<06:01, 1836.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 20000/666037 [00:02<00:31, 20299.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  10%|█         | 69000/666037 [00:02<00:12, 49174.09 examples/s]Running bart tokenizers on train dataset (num_proc=51):   9%|▉         | 63000/666037 [00:02<00:13, 43829.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▍         | 31000/666037 [00:02<00:24, 25723.42 examples/s]Running bart tokenizers on train dataset (num_proc=51):   9%|▉         | 61000/666037 [00:02<00:15, 39282.43 examples/s]Running bart tokenizers on train dataset (num_proc=51):   9%|▉         | 63000/666037 [00:02<00:12, 49024.04 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|          | 8000/666037 [00:02<01:48, 6037.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):   4%|▍         | 26000/666037 [00:02<00:22, 27904.40 examples/s]Running bart tokenizers on train dataset (num_proc=51):  11%|█▏        | 76000/666037 [00:02<00:11, 52039.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  10%|▉         | 66000/666037 [00:02<00:14, 41358.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▌         | 35000/666037 [00:02<00:24, 26098.84 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 11000/666037 [00:02<01:15, 8630.21 examples/s]Running bart tokenizers on train dataset (num_proc=51):  10%|█         | 69000/666037 [00:02<00:15, 39011.42 examples/s]Running bart tokenizers on train dataset (num_proc=51):  11%|█         | 72000/666037 [00:02<00:13, 45288.50 examples/s]Running bart tokenizers on train dataset (num_proc=51):  10%|█         | 69000/666037 [00:02<00:15, 38683.04 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 16000/666037 [00:02<00:46, 14035.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):   6%|▋         | 43000/666037 [00:02<00:16, 36846.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▍         | 30000/666037 [00:02<00:25, 25156.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 77000/666037 [00:02<00:12, 46610.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 82000/666037 [00:02<00:13, 42183.54 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 79000/666037 [00:02<00:11, 49718.28 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▌         | 36000/666037 [00:02<00:19, 32465.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):  11%|█         | 74000/666037 [00:02<00:15, 38594.09 examples/s]Running bart tokenizers on train dataset (num_proc=51):   7%|▋         | 48000/666037 [00:02<00:17, 36344.10 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 20000/666037 [00:02<00:38, 16698.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 83000/666037 [00:02<00:13, 43531.02 examples/s]Running bart tokenizers on train dataset (num_proc=51):  13%|█▎        | 86000/666037 [00:02<00:11, 49303.74 examples/s]Running bart tokenizers on train dataset (num_proc=51):   6%|▌         | 41000/666037 [00:02<00:17, 34912.15 examples/s]Running bart tokenizers on train dataset (num_proc=51):   4%|▍         | 25000/666037 [00:02<00:29, 22016.33 examples/s]Running bart tokenizers on train dataset (num_proc=51):   9%|▊         | 57000/666037 [00:02<00:13, 45609.41 examples/s]Running bart tokenizers on train dataset (num_proc=51):  13%|█▎        | 87000/666037 [00:03<00:17, 33321.97 examples/s]Running bart tokenizers on train dataset (num_proc=51):  13%|█▎        | 89000/666037 [00:02<00:12, 46843.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 79000/666037 [00:02<00:16, 35747.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):   7%|▋         | 48000/666037 [00:02<00:14, 41966.02 examples/s]Running bart tokenizers on train dataset (num_proc=51):  14%|█▍        | 92000/666037 [00:03<00:12, 47080.38 examples/s]Running bart tokenizers on train dataset (num_proc=51):  10%|▉         | 65000/666037 [00:02<00:11, 52970.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):   4%|▍         | 29000/666037 [00:02<00:25, 24963.15 examples/s]Running bart tokenizers on train dataset (num_proc=51):  13%|█▎        | 88000/666037 [00:03<00:12, 46176.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  15%|█▍        | 97000/666037 [00:03<00:13, 43532.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):   8%|▊         | 54000/666037 [00:02<00:13, 46285.55 examples/s]Running bart tokenizers on train dataset (num_proc=51):  14%|█▍        | 95000/666037 [00:03<00:12, 44157.33 examples/s]Running bart tokenizers on train dataset (num_proc=51):  15%|█▍        | 97000/666037 [00:03<00:13, 42769.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▌         | 34000/666037 [00:02<00:21, 28853.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  15%|█▍        | 97000/666037 [00:03<00:10, 56115.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  15%|█▌        | 103000/666037 [00:03<00:12, 44170.06 examples/s]Running bart tokenizers on train dataset (num_proc=51):  15%|█▌        | 102000/666037 [00:03<00:11, 49538.06 examples/s]Running bart tokenizers on train dataset (num_proc=51):   9%|▉         | 60000/666037 [00:03<00:13, 44646.56 examples/s]Running bart tokenizers on train dataset (num_proc=51):  15%|█▌        | 102000/666037 [00:03<00:12, 44097.42 examples/s]Running bart tokenizers on train dataset (num_proc=51):   6%|▌         | 40000/666037 [00:02<00:17, 35883.14 examples/s]Running bart tokenizers on train dataset (num_proc=51):  16%|█▌        | 104000/666037 [00:03<00:10, 56023.37 examples/s]Running bart tokenizers on train dataset (num_proc=51):  16%|█▌        | 108000/666037 [00:03<00:11, 50275.36 examples/s]Running bart tokenizers on train dataset (num_proc=51):  11%|█         | 71000/666037 [00:03<00:16, 36837.87 examples/s]Running bart tokenizers on train dataset (num_proc=51):  10%|█         | 67000/666037 [00:03<00:11, 50117.18 examples/s]Running bart tokenizers on train dataset (num_proc=51):  16%|█▌        | 108000/666037 [00:03<00:13, 40900.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  16%|█▌        | 107000/666037 [00:03<00:13, 41761.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):   7%|▋         | 46000/666037 [00:03<00:16, 37118.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 77000/666037 [00:03<00:14, 41205.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):  17%|█▋        | 114000/666037 [00:03<00:12, 44603.29 examples/s]Running bart tokenizers on train dataset (num_proc=51):  17%|█▋        | 111000/666037 [00:03<00:10, 50507.50 examples/s]Running bart tokenizers on train dataset (num_proc=51):  17%|█▋        | 114000/666037 [00:03<00:11, 47224.38 examples/s]Running bart tokenizers on train dataset (num_proc=51):   8%|▊         | 51000/666037 [00:03<00:15, 39006.35 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 121000/666037 [00:03<00:10, 49831.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  11%|█         | 73000/666037 [00:03<00:15, 37423.05 examples/s]Running bart tokenizers on train dataset (num_proc=51):  17%|█▋        | 112000/666037 [00:03<00:16, 34324.55 examples/s]Running bart tokenizers on train dataset (num_proc=51):   9%|▉         | 59000/666037 [00:03<00:12, 48924.10 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 119000/666037 [00:03<00:13, 39866.64 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 83000/666037 [00:03<00:16, 34323.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 0/666037 [00:00<?, ? examples/s]Running bart tokenizers on train dataset (num_proc=51):  19%|█▉        | 127000/666037 [00:03<00:11, 46410.52 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 117000/666037 [00:03<00:15, 36230.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 78000/666037 [00:03<00:16, 36417.52 examples/s]Running bart tokenizers on train dataset (num_proc=51):  19%|█▊        | 124000/666037 [00:03<00:13, 39226.77 examples/s]Running bart tokenizers on train dataset (num_proc=51):  10%|▉         | 65000/666037 [00:03<00:13, 45205.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  13%|█▎        | 89000/666037 [00:03<00:15, 37337.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 117000/666037 [00:03<00:16, 32998.69 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 123000/666037 [00:03<00:13, 41288.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  13%|█▎        | 84000/666037 [00:03<00:14, 40413.20 examples/s]Running bart tokenizers on train dataset (num_proc=51):  19%|█▉        | 129000/666037 [00:03<00:13, 39086.76 examples/s]Running bart tokenizers on train dataset (num_proc=51):  14%|█▍        | 95000/666037 [00:03<00:14, 40751.95 examples/s]Running bart tokenizers on train dataset (num_proc=51):  19%|█▉        | 128000/666037 [00:03<00:12, 43200.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  11%|█         | 70000/666037 [00:03<00:15, 39293.41 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 123000/666037 [00:03<00:15, 35305.40 examples/s]Running bart tokenizers on train dataset (num_proc=51):  20%|█▉        | 132000/666037 [00:04<00:15, 35285.82 examples/s]Running bart tokenizers on train dataset (num_proc=51):  13%|█▎        | 89000/666037 [00:03<00:14, 38574.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  20%|██        | 134000/666037 [00:04<00:13, 40531.11 examples/s]Running bart tokenizers on train dataset (num_proc=51):  15%|█▌        | 101000/666037 [00:03<00:12, 43563.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):  20%|█▉        | 133000/666037 [00:04<00:12, 43632.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  21%|██        | 137000/666037 [00:04<00:14, 37145.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):  19%|█▉        | 128000/666037 [00:04<00:15, 35800.78 examples/s]Running bart tokenizers on train dataset (num_proc=51):  15%|█▍        | 97000/666037 [00:04<00:12, 44317.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  21%|██        | 138000/666037 [00:04<00:11, 44962.62 examples/s]Running bart tokenizers on train dataset (num_proc=51):  16%|█▌        | 107000/666037 [00:04<00:12, 45625.33 examples/s]Running bart tokenizers on train dataset (num_proc=51):  21%|██        | 139000/666037 [00:04<00:13, 39853.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  21%|██▏       | 142000/666037 [00:04<00:13, 37879.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  20%|█▉        | 133000/666037 [00:04<00:14, 37689.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  11%|█▏        | 75000/666037 [00:03<00:18, 31109.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  22%|██▏       | 144000/666037 [00:04<00:10, 47757.83 examples/s]Running bart tokenizers on train dataset (num_proc=51):  16%|█▌        | 104000/666037 [00:04<00:11, 48939.94 examples/s]Running bart tokenizers on train dataset (num_proc=51):  17%|█▋        | 115000/666037 [00:04<00:10, 53183.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  22%|██▏       | 147000/666037 [00:04<00:11, 45791.15 examples/s]Running bart tokenizers on train dataset (num_proc=51):  22%|██▏       | 149000/666037 [00:04<00:11, 44891.50 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 82000/666037 [00:04<00:15, 38012.85 examples/s]Running bart tokenizers on train dataset (num_proc=51):  22%|██▏       | 145000/666037 [00:04<00:10, 50791.04 examples/s]Running bart tokenizers on train dataset (num_proc=51):  17%|█▋        | 111000/666037 [00:04<00:10, 53312.53 examples/s]Running bart tokenizers on train dataset (num_proc=51):  23%|██▎       | 150000/666037 [00:04<00:10, 48820.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  23%|██▎       | 152000/666037 [00:04<00:11, 45474.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  23%|██▎       | 154000/666037 [00:04<00:11, 44381.64 examples/s]Running bart tokenizers on train dataset (num_proc=51):  13%|█▎        | 88000/666037 [00:04<00:14, 40269.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  23%|██▎       | 151000/666037 [00:04<00:09, 52048.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 121000/666037 [00:04<00:12, 42890.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 117000/666037 [00:04<00:10, 51437.00 examples/s]Running bart tokenizers on train dataset (num_proc=51):  24%|██▎       | 157000/666037 [00:04<00:11, 45571.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  24%|██▍       | 159000/666037 [00:04<00:11, 43090.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  14%|█▍        | 94000/666037 [00:04<00:13, 43771.27 examples/s]Running bart tokenizers on train dataset (num_proc=51):  19%|█▉        | 126000/666037 [00:04<00:12, 42326.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  24%|██▎       | 157000/666037 [00:04<00:10, 46974.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  23%|██▎       | 155000/666037 [00:04<00:14, 35267.37 examples/s]Running bart tokenizers on train dataset (num_proc=51):  24%|██▍       | 162000/666037 [00:04<00:11, 43573.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  15%|█▌        | 102000/666037 [00:04<00:11, 50721.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  25%|██▌       | 167000/666037 [00:04<00:10, 46047.50 examples/s]Running bart tokenizers on train dataset (num_proc=51):  24%|██▍       | 162000/666037 [00:04<00:10, 46557.16 examples/s]Running bart tokenizers on train dataset (num_proc=51):  24%|██▍       | 160000/666037 [00:04<00:13, 36884.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 123000/666037 [00:04<00:14, 36873.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  20%|█▉        | 131000/666037 [00:04<00:14, 36724.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  16%|█▌        | 108000/666037 [00:04<00:11, 50122.43 examples/s]Running bart tokenizers on train dataset (num_proc=51):  26%|██▌       | 174000/666037 [00:04<00:09, 50859.75 examples/s]Running bart tokenizers on train dataset (num_proc=51):  25%|██▌       | 167000/666037 [00:04<00:13, 36296.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  19%|█▉        | 128000/666037 [00:04<00:14, 35876.54 examples/s]Running bart tokenizers on train dataset (num_proc=51):  25%|██▍       | 165000/666037 [00:04<00:15, 32851.84 examples/s]Running bart tokenizers on train dataset (num_proc=51):  20%|██        | 136000/666037 [00:04<00:15, 34281.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  26%|██▌       | 173000/666037 [00:04<00:12, 39773.69 examples/s]Running bart tokenizers on train dataset (num_proc=51):  17%|█▋        | 114000/666037 [00:04<00:12, 43371.50 examples/s]Running bart tokenizers on train dataset (num_proc=51):  25%|██▌       | 169000/666037 [00:05<00:14, 34073.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 180000/666037 [00:05<00:12, 39129.43 examples/s]Running bart tokenizers on train dataset (num_proc=51):  25%|██▌       | 167000/666037 [00:05<00:16, 30788.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  21%|██        | 140000/666037 [00:04<00:14, 35129.74 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 178000/666037 [00:05<00:11, 41147.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  20%|██        | 134000/666037 [00:04<00:14, 37162.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  22%|██▏       | 145000/666037 [00:05<00:13, 37278.16 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 119000/666037 [00:04<00:15, 36385.11 examples/s]Running bart tokenizers on train dataset (num_proc=51):  21%|██        | 139000/666037 [00:05<00:13, 37912.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  26%|██▌       | 173000/666037 [00:05<00:14, 33452.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  28%|██▊       | 185000/666037 [00:05<00:13, 36906.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 183000/666037 [00:05<00:13, 35296.16 examples/s]Running bart tokenizers on train dataset (num_proc=51):  26%|██▋       | 175000/666037 [00:05<00:15, 30946.89 examples/s]Running bart tokenizers on train dataset (num_proc=51):  23%|██▎       | 154000/666037 [00:05<00:10, 46949.05 examples/s]Running bart tokenizers on train dataset (num_proc=51):  22%|██▏       | 144000/666037 [00:05<00:12, 40182.53 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 181000/666037 [00:05<00:13, 36081.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  28%|██▊       | 188000/666037 [00:05<00:13, 35405.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 178000/666037 [00:05<00:16, 30344.87 examples/s]Running bart tokenizers on train dataset (num_proc=51):  19%|█▊        | 124000/666037 [00:05<00:17, 30551.15 examples/s]Running bart tokenizers on train dataset (num_proc=51):  22%|██▏       | 149000/666037 [00:05<00:13, 39084.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▊       | 190000/666037 [00:05<00:15, 31217.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  24%|██▍       | 161000/666037 [00:05<00:10, 46373.53 examples/s]Running bart tokenizers on train dataset (num_proc=51):  28%|██▊       | 188000/666037 [00:05<00:11, 43340.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 183000/666037 [00:05<00:14, 33945.74 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▉       | 195000/666037 [00:05<00:13, 34711.76 examples/s]Running bart tokenizers on train dataset (num_proc=51):  23%|██▎       | 154000/666037 [00:05<00:12, 41123.43 examples/s]Running bart tokenizers on train dataset (num_proc=51):  25%|██▍       | 166000/666037 [00:05<00:11, 43608.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  20%|█▉        | 132000/666037 [00:05<00:15, 35037.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▉       | 192000/666037 [00:05<00:15, 30018.85 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▊       | 190000/666037 [00:05<00:12, 39264.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  30%|███       | 201000/666037 [00:05<00:11, 39329.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▉       | 193000/666037 [00:05<00:12, 38493.42 examples/s]Running bart tokenizers on train dataset (num_proc=51):  24%|██▍       | 162000/666037 [00:05<00:10, 49360.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  26%|██▌       | 171000/666037 [00:05<00:11, 44757.84 examples/s]Running bart tokenizers on train dataset (num_proc=51):  30%|███       | 201000/666037 [00:05<00:11, 39866.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  20%|██        | 136000/666037 [00:05<00:15, 33387.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  30%|██▉       | 199000/666037 [00:05<00:09, 49048.23 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 1000/666037 [00:02<23:30, 471.64 examples/s]Running bart tokenizers on train dataset (num_proc=51):  31%|███       | 206000/666037 [00:05<00:12, 37119.30 examples/s]Running bart tokenizers on train dataset (num_proc=51):  25%|██▌       | 168000/666037 [00:05<00:11, 43877.78 examples/s]Running bart tokenizers on train dataset (num_proc=51):  31%|███       | 206000/666037 [00:05<00:11, 39595.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):  30%|██▉       | 198000/666037 [00:05<00:15, 31008.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  21%|██        | 141000/666037 [00:05<00:16, 31859.52 examples/s]Running bart tokenizers on train dataset (num_proc=51):  26%|██▋       | 176000/666037 [00:05<00:13, 35817.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):  31%|███       | 205000/666037 [00:05<00:10, 45159.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 211000/666037 [00:05<00:11, 38790.63 examples/s]Running bart tokenizers on train dataset (num_proc=51):   0%|          | 3000/666037 [00:02<06:32, 1687.41 examples/s]Running bart tokenizers on train dataset (num_proc=51):  26%|██▌       | 173000/666037 [00:05<00:12, 40642.11 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 211000/666037 [00:06<00:12, 37483.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):  22%|██▏       | 146000/666037 [00:05<00:14, 35305.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  31%|███       | 204000/666037 [00:06<00:13, 34932.02 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 183000/666037 [00:05<00:11, 40856.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 216000/666037 [00:06<00:11, 39291.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 210000/666037 [00:06<00:10, 41661.65 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|          | 5000/666037 [00:02<03:35, 3067.35 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 215000/666037 [00:06<00:11, 37899.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  23%|██▎       | 154000/666037 [00:05<00:11, 43951.89 examples/s]Running bart tokenizers on train dataset (num_proc=51):  31%|███▏      | 209000/666037 [00:06<00:12, 37183.85 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 178000/666037 [00:06<00:12, 37567.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▎      | 224000/666037 [00:06<00:09, 47490.76 examples/s]Running bart tokenizers on train dataset (num_proc=51):   1%|▏         | 9000/666037 [00:02<01:37, 6748.82 examples/s]Running bart tokenizers on train dataset (num_proc=51):  28%|██▊       | 188000/666037 [00:06<00:13, 35479.94 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 214000/666037 [00:06<00:11, 39054.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 183000/666037 [00:06<00:12, 38531.26 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▍      | 229000/666037 [00:06<00:09, 47264.97 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 219000/666037 [00:06<00:13, 33756.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 216000/666037 [00:06<00:13, 33475.76 examples/s]Running bart tokenizers on train dataset (num_proc=51):   2%|▏         | 15000/666037 [00:02<00:51, 12709.76 examples/s]Running bart tokenizers on train dataset (num_proc=51):  24%|██▍       | 159000/666037 [00:06<00:14, 36111.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▉       | 192000/666037 [00:06<00:14, 33464.16 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▎      | 224000/666037 [00:06<00:12, 36229.78 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 219000/666037 [00:06<00:11, 37767.57 examples/s]Running bart tokenizers on train dataset (num_proc=51):  28%|██▊       | 188000/666037 [00:06<00:13, 35507.25 examples/s]Running bart tokenizers on train dataset (num_proc=51):  25%|██▍       | 166000/666037 [00:06<00:11, 42479.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 220000/666037 [00:06<00:13, 31919.05 examples/s]Running bart tokenizers on train dataset (num_proc=51):  30%|██▉       | 197000/666037 [00:06<00:13, 36066.42 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▍      | 229000/666037 [00:06<00:11, 36930.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 19000/666037 [00:02<00:44, 14700.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▌      | 234000/666037 [00:06<00:12, 33538.28 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▍      | 225000/666037 [00:06<00:11, 38528.21 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▉       | 192000/666037 [00:06<00:13, 34666.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  26%|██▌       | 172000/666037 [00:06<00:11, 43675.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  30%|███       | 201000/666037 [00:06<00:12, 36775.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▎      | 224000/666037 [00:06<00:14, 30861.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):   3%|▎         | 23000/666037 [00:02<00:34, 18553.77 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▍      | 233000/666037 [00:06<00:13, 32332.27 examples/s]Running bart tokenizers on train dataset (num_proc=51):  31%|███       | 206000/666037 [00:06<00:11, 39388.62 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▉       | 196000/666037 [00:06<00:14, 31982.55 examples/s]Running bart tokenizers on train dataset (num_proc=51):   4%|▍         | 27000/666037 [00:03<00:28, 22214.78 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▌      | 238000/666037 [00:06<00:14, 28841.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▍      | 230000/666037 [00:06<00:13, 32806.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 177000/666037 [00:06<00:13, 37097.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▍      | 228000/666037 [00:06<00:15, 27595.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  30%|███       | 201000/666037 [00:06<00:12, 35900.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▌      | 237000/666037 [00:06<00:14, 30241.65 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▍         | 32000/666037 [00:03<00:23, 26818.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 211000/666037 [00:06<00:12, 36383.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▌      | 235000/666037 [00:06<00:12, 34465.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 182000/666037 [00:06<00:12, 38536.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▌      | 236000/666037 [00:06<00:11, 37670.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  31%|███       | 207000/666037 [00:06<00:11, 39007.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▌      | 239000/666037 [00:07<00:12, 35396.54 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▋      | 242000/666037 [00:07<00:17, 24027.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):   5%|▌         | 36000/666037 [00:03<00:23, 26691.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 217000/666037 [00:06<00:11, 38248.75 examples/s]Running bart tokenizers on train dataset (num_proc=51):  28%|██▊       | 188000/666037 [00:06<00:11, 41524.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▌      | 241000/666037 [00:07<00:15, 27448.00 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 212000/666037 [00:06<00:11, 40390.62 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▌      | 241000/666037 [00:07<00:12, 35169.69 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 245000/666037 [00:07<00:17, 24104.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 222000/666037 [00:07<00:11, 37823.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):   6%|▋         | 42000/666037 [00:03<00:20, 31018.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▋      | 243000/666037 [00:07<00:13, 31698.95 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 245000/666037 [00:07<00:14, 28217.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▉       | 193000/666037 [00:06<00:12, 38400.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 218000/666037 [00:07<00:10, 44235.07 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 246000/666037 [00:07<00:10, 38241.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):   8%|▊         | 50000/666037 [00:03<00:15, 39038.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▍      | 227000/666037 [00:07<00:11, 37044.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 248000/666037 [00:07<00:20, 20850.21 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 223000/666037 [00:07<00:11, 40056.87 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 250000/666037 [00:07<00:15, 27506.75 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 247000/666037 [00:07<00:16, 25855.54 examples/s]Running bart tokenizers on train dataset (num_proc=51):  30%|██▉       | 198000/666037 [00:07<00:14, 31840.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):   8%|▊         | 55000/666037 [00:03<00:17, 35387.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▍      | 228000/666037 [00:07<00:11, 37663.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):  30%|███       | 203000/666037 [00:07<00:13, 33591.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▍      | 232000/666037 [00:07<00:14, 30090.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 251000/666037 [00:07<00:22, 18575.76 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 251000/666037 [00:07<00:16, 25139.27 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 251000/666037 [00:07<00:16, 24523.70 examples/s]Running bart tokenizers on train dataset (num_proc=51):   9%|▉         | 59000/666037 [00:03<00:17, 34864.40 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▍      | 232000/666037 [00:07<00:12, 34145.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  31%|███       | 207000/666037 [00:07<00:14, 32402.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▌      | 236000/666037 [00:07<00:15, 27425.98 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 253000/666037 [00:07<00:23, 17642.74 examples/s]Running bart tokenizers on train dataset (num_proc=51):   9%|▉         | 63000/666037 [00:04<00:18, 31957.54 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 212000/666037 [00:07<00:12, 35406.54 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▌      | 236000/666037 [00:07<00:13, 30888.77 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▌      | 239000/666037 [00:07<00:15, 27753.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):  10%|█         | 67000/666037 [00:04<00:18, 32792.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 254000/666037 [00:07<00:23, 17896.33 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 217000/666037 [00:07<00:13, 32438.11 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▌      | 240000/666037 [00:07<00:15, 27929.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 244000/666037 [00:07<00:15, 27970.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 254000/666037 [00:08<00:33, 12432.15 examples/s]Running bart tokenizers on train dataset (num_proc=51):  11%|█         | 73000/666037 [00:04<00:17, 33004.00 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 255000/666037 [00:08<00:25, 15927.57 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 221000/666037 [00:07<00:14, 31481.52 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 256000/666037 [00:08<00:29, 13754.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▋      | 243000/666037 [00:08<00:16, 25511.55 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 247000/666037 [00:08<00:16, 26020.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 78000/666037 [00:04<00:17, 34434.78 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▊      | 257000/666037 [00:08<00:29, 13650.35 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 248000/666037 [00:08<00:13, 29924.05 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▊      | 258000/666037 [00:08<00:29, 13689.15 examples/s]Running bart tokenizers on train dataset (num_proc=51):  12%|█▏        | 83000/666037 [00:04<00:15, 36662.65 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|███▉      | 264000/666037 [00:08<00:18, 21561.36 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▍      | 225000/666037 [00:08<00:17, 25716.78 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▊      | 258000/666037 [00:08<00:27, 14716.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 250000/666037 [00:08<00:20, 20726.70 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 261000/666037 [00:08<00:25, 15592.30 examples/s]Running bart tokenizers on train dataset (num_proc=51):  13%|█▎        | 88000/666037 [00:04<00:14, 38549.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▊      | 257000/666037 [00:08<00:34, 11894.62 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|████      | 267000/666037 [00:08<00:19, 20532.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  14%|█▍        | 92000/666037 [00:04<00:14, 38290.87 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▍      | 230000/666037 [00:08<00:16, 26686.87 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 263000/666037 [00:08<00:28, 14302.55 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 259000/666037 [00:08<00:34, 11783.59 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 271000/666037 [00:08<00:17, 22920.25 examples/s]Running bart tokenizers on train dataset (num_proc=51):  14%|█▍        | 96000/666037 [00:04<00:15, 37986.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 253000/666037 [00:08<00:24, 16772.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|████      | 269000/666037 [00:08<00:17, 22223.26 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▍      | 233000/666037 [00:08<00:18, 23815.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 252000/666037 [00:08<00:23, 17908.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 261000/666037 [00:08<00:31, 12733.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 261000/666037 [00:08<00:34, 11696.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  15%|█▌        | 103000/666037 [00:05<00:14, 40099.97 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 273000/666037 [00:08<00:15, 25557.92 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▌      | 237000/666037 [00:08<00:16, 26731.73 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|███▉      | 264000/666037 [00:08<00:27, 14547.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 274000/666037 [00:08<00:21, 18644.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  16%|█▌        | 107000/666037 [00:05<00:14, 38960.29 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 256000/666037 [00:08<00:21, 18704.69 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▌      | 240000/666037 [00:08<00:15, 27199.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 255000/666037 [00:08<00:29, 13735.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|███▉      | 264000/666037 [00:08<00:30, 13303.92 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 277000/666037 [00:08<00:15, 25049.45 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 278000/666037 [00:09<00:17, 21714.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 244000/666037 [00:08<00:14, 30008.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|███▉      | 266000/666037 [00:09<00:30, 13064.65 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|████      | 267000/666037 [00:09<00:29, 13529.07 examples/s]Running bart tokenizers on train dataset (num_proc=51):  17%|█▋        | 111000/666037 [00:05<00:18, 29544.36 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 280000/666037 [00:09<00:17, 22436.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 281000/666037 [00:09<00:18, 20326.78 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 248000/666037 [00:08<00:15, 26576.98 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 270000/666037 [00:09<00:22, 17729.64 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 270000/666037 [00:09<00:26, 15146.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 259000/666037 [00:09<00:27, 14836.36 examples/s]Running bart tokenizers on train dataset (num_proc=51):  17%|█▋        | 116000/666037 [00:05<00:17, 31916.53 examples/s]Running bart tokenizers on train dataset (num_proc=51):  43%|████▎     | 289000/666037 [00:09<00:11, 33427.56 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 272000/666037 [00:09<00:26, 14877.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):  43%|████▎     | 284000/666037 [00:09<00:21, 17641.35 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 251000/666037 [00:09<00:18, 22714.23 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 263000/666037 [00:09<00:23, 17164.15 examples/s]Running bart tokenizers on train dataset (num_proc=51):  18%|█▊        | 123000/666037 [00:05<00:14, 37379.33 examples/s]Running bart tokenizers on train dataset (num_proc=51):  44%|████▍     | 293000/666037 [00:09<00:11, 33095.05 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 273000/666037 [00:09<00:25, 15626.27 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 280000/666037 [00:09<00:14, 25960.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  44%|████▍     | 293000/666037 [00:09<00:12, 30432.59 examples/s]Running bart tokenizers on train dataset (num_proc=51):  19%|█▉        | 127000/666037 [00:05<00:14, 37045.43 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 254000/666037 [00:09<00:17, 23163.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|████      | 267000/666037 [00:09<00:20, 19276.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▍     | 297000/666037 [00:09<00:11, 32121.92 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████▏     | 276000/666037 [00:09<00:23, 16598.87 examples/s]Running bart tokenizers on train dataset (num_proc=51):  20%|█▉        | 132000/666037 [00:05<00:13, 39543.54 examples/s]Running bart tokenizers on train dataset (num_proc=51):  43%|████▎     | 284000/666037 [00:09<00:13, 27378.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 270000/666037 [00:09<00:18, 20918.36 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▌     | 301000/666037 [00:09<00:10, 33366.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▍     | 297000/666037 [00:09<00:13, 27302.02 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▊      | 257000/666037 [00:09<00:19, 20939.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  21%|██        | 137000/666037 [00:06<00:13, 38033.13 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▊      | 257000/666037 [00:09<01:01, 6670.47 examples/s] Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 279000/666037 [00:09<00:23, 16593.21 examples/s]Running bart tokenizers on train dataset (num_proc=51):  43%|████▎     | 288000/666037 [00:09<00:14, 25659.75 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 260000/666037 [00:09<00:18, 21379.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▌     | 301000/666037 [00:09<00:13, 27225.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  46%|████▌     | 305000/666037 [00:09<00:12, 28988.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 273000/666037 [00:09<00:20, 18724.06 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 259000/666037 [00:09<00:51, 7899.06 examples/s]Running bart tokenizers on train dataset (num_proc=51):  43%|████▎     | 284000/666037 [00:09<00:16, 22507.75 examples/s]Running bart tokenizers on train dataset (num_proc=51):  21%|██        | 141000/666037 [00:06<00:15, 33207.27 examples/s]Running bart tokenizers on train dataset (num_proc=51):  44%|████▍     | 293000/666037 [00:09<00:12, 29185.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  47%|████▋     | 314000/666037 [00:09<00:08, 41687.97 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 277000/666037 [00:09<00:18, 21186.14 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 263000/666037 [00:09<00:20, 19885.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|████      | 268000/666037 [00:09<00:23, 17102.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):  44%|████▎     | 290000/666037 [00:10<00:13, 28619.05 examples/s]Running bart tokenizers on train dataset (num_proc=51):  22%|██▏       | 145000/666037 [00:06<00:15, 34348.98 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▍     | 299000/666037 [00:10<00:10, 33991.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  48%|████▊     | 320000/666037 [00:10<00:07, 44768.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  46%|████▌     | 305000/666037 [00:10<00:15, 22931.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 282000/666037 [00:09<00:14, 26749.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|███▉      | 266000/666037 [00:09<00:18, 21871.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  23%|██▎       | 151000/666037 [00:06<00:12, 40645.55 examples/s]Running bart tokenizers on train dataset (num_proc=51):  46%|████▌     | 304000/666037 [00:10<00:09, 37302.52 examples/s]Running bart tokenizers on train dataset (num_proc=51):  47%|████▋     | 310000/666037 [00:10<00:12, 27843.10 examples/s]Running bart tokenizers on train dataset (num_proc=51):  43%|████▎     | 287000/666037 [00:10<00:12, 31544.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|████      | 269000/666037 [00:09<00:17, 23018.77 examples/s]Running bart tokenizers on train dataset (num_proc=51):  24%|██▍       | 159000/666037 [00:06<00:10, 49598.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  49%|████▉     | 325000/666037 [00:10<00:09, 37540.36 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 272000/666037 [00:10<00:23, 16659.52 examples/s]Running bart tokenizers on train dataset (num_proc=51):  44%|████▍     | 294000/666037 [00:10<00:16, 22631.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  44%|████▎     | 291000/666037 [00:10<00:11, 32593.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 282000/666037 [00:10<00:13, 28435.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  46%|████▋     | 309000/666037 [00:10<00:11, 30229.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▍     | 299000/666037 [00:10<00:13, 27039.92 examples/s]Running bart tokenizers on train dataset (num_proc=51):  50%|████▉     | 330000/666037 [00:10<00:09, 34508.83 examples/s]Running bart tokenizers on train dataset (num_proc=51):  47%|████▋     | 314000/666037 [00:10<00:15, 23117.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  25%|██▍       | 165000/666037 [00:06<00:12, 40480.94 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 272000/666037 [00:10<00:20, 18831.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):  44%|████▍     | 295000/666037 [00:10<00:11, 31456.27 examples/s]Running bart tokenizers on train dataset (num_proc=51):  47%|████▋     | 313000/666037 [00:10<00:11, 31781.59 examples/s]Running bart tokenizers on train dataset (num_proc=51):  44%|████▍     | 292000/666037 [00:10<00:10, 35966.36 examples/s]Running bart tokenizers on train dataset (num_proc=51):  48%|████▊     | 322000/666037 [00:10<00:10, 32846.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████▏     | 275000/666037 [00:10<00:18, 20937.06 examples/s]Running bart tokenizers on train dataset (num_proc=51):  26%|██▌       | 170000/666037 [00:06<00:12, 39859.05 examples/s]Running bart tokenizers on train dataset (num_proc=51):  51%|█████     | 337000/666037 [00:10<00:08, 37826.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  46%|████▌     | 304000/666037 [00:10<00:13, 27345.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▍     | 299000/666037 [00:10<00:12, 30207.37 examples/s]Running bart tokenizers on train dataset (num_proc=51):  48%|████▊     | 320000/666037 [00:10<00:09, 37740.04 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▍     | 298000/666037 [00:10<00:09, 39961.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):  46%|████▌     | 308000/666037 [00:10<00:12, 29410.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  49%|████▉     | 327000/666037 [00:10<00:10, 32405.59 examples/s]Running bart tokenizers on train dataset (num_proc=51):  26%|██▋       | 175000/666037 [00:06<00:12, 38808.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 278000/666037 [00:10<00:19, 19994.89 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▌     | 303000/666037 [00:10<00:12, 30014.16 examples/s]Running bart tokenizers on train dataset (num_proc=51):  49%|████▉     | 325000/666037 [00:10<00:08, 39004.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  52%|█████▏    | 346000/666037 [00:10<00:07, 42003.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):  47%|████▋     | 312000/666037 [00:10<00:11, 31232.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):  50%|█████     | 334000/666037 [00:10<00:08, 39345.70 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 282000/666037 [00:10<00:15, 24369.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  46%|████▌     | 307000/666037 [00:10<00:11, 30615.41 examples/s]Running bart tokenizers on train dataset (num_proc=51):  27%|██▋       | 180000/666037 [00:07<00:13, 37119.20 examples/s]Running bart tokenizers on train dataset (num_proc=51):  48%|████▊     | 318000/666037 [00:10<00:09, 37891.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  51%|█████     | 340000/666037 [00:10<00:07, 43466.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  50%|████▉     | 330000/666037 [00:10<00:09, 35062.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 351000/666037 [00:10<00:08, 36861.85 examples/s]Running bart tokenizers on train dataset (num_proc=51):  28%|██▊       | 186000/666037 [00:07<00:11, 40656.59 examples/s]Running bart tokenizers on train dataset (num_proc=51):  43%|████▎     | 285000/666037 [00:10<00:18, 21006.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):  52%|█████▏    | 345000/666037 [00:11<00:07, 42487.84 examples/s]Running bart tokenizers on train dataset (num_proc=51):  49%|████▉     | 327000/666037 [00:11<00:07, 45716.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):  47%|████▋     | 313000/666037 [00:10<00:11, 30180.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  50%|█████     | 335000/666037 [00:11<00:09, 35359.75 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 356000/666037 [00:11<00:08, 37331.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  43%|████▎     | 288000/666037 [00:10<00:17, 21574.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  50%|█████     | 334000/666037 [00:11<00:06, 49676.73 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 350000/666037 [00:11<00:07, 42046.38 examples/s]Running bart tokenizers on train dataset (num_proc=51):  46%|████▌     | 304000/666037 [00:11<00:15, 23156.69 examples/s]Running bart tokenizers on train dataset (num_proc=51):  48%|████▊     | 318000/666037 [00:11<00:10, 32523.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▊       | 191000/666037 [00:07<00:14, 32832.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  51%|█████     | 339000/666037 [00:11<00:10, 32691.30 examples/s]Running bart tokenizers on train dataset (num_proc=51):  54%|█████▍    | 360000/666037 [00:11<00:09, 33007.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  44%|████▍     | 294000/666037 [00:10<00:13, 27732.73 examples/s]Running bart tokenizers on train dataset (num_proc=51):  48%|████▊     | 322000/666037 [00:11<00:10, 33422.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  46%|████▋     | 309000/666037 [00:11<00:13, 25577.84 examples/s]Running bart tokenizers on train dataset (num_proc=51):  55%|█████▍    | 365000/666037 [00:11<00:08, 36370.21 examples/s]Running bart tokenizers on train dataset (num_proc=51):  29%|██▉       | 195000/666037 [00:07<00:15, 31164.19 examples/s]Running bart tokenizers on train dataset (num_proc=51):  52%|█████▏    | 345000/666037 [00:11<00:09, 34875.43 examples/s]Running bart tokenizers on train dataset (num_proc=51):  51%|█████     | 340000/666037 [00:11<00:08, 38205.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▍     | 298000/666037 [00:11<00:13, 28176.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  47%|████▋     | 316000/666037 [00:11<00:10, 31977.23 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 355000/666037 [00:11<00:10, 30745.16 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▌    | 370000/666037 [00:11<00:07, 38753.30 examples/s]Running bart tokenizers on train dataset (num_proc=51):  49%|████▉     | 326000/666037 [00:11<00:10, 31022.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  30%|██▉       | 199000/666037 [00:07<00:14, 32739.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  52%|█████▏    | 349000/666037 [00:11<00:09, 32447.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  46%|████▌     | 306000/666037 [00:11<00:09, 38102.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  54%|█████▍    | 360000/666037 [00:11<00:09, 33469.20 examples/s]Running bart tokenizers on train dataset (num_proc=51):  50%|████▉     | 333000/666037 [00:11<00:08, 38650.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  31%|███       | 208000/666037 [00:07<00:10, 45180.73 examples/s]Running bart tokenizers on train dataset (num_proc=51):  48%|████▊     | 321000/666037 [00:11<00:11, 30877.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 353000/666037 [00:11<00:09, 33882.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▋    | 375000/666037 [00:11<00:09, 31567.28 examples/s]Running bart tokenizers on train dataset (num_proc=51):  51%|█████     | 338000/666037 [00:11<00:08, 40111.94 examples/s]Running bart tokenizers on train dataset (num_proc=51):  55%|█████▍    | 365000/666037 [00:11<00:08, 33879.16 examples/s]Running bart tokenizers on train dataset (num_proc=51):  47%|████▋     | 310000/666037 [00:11<00:10, 33367.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  49%|████▉     | 327000/666037 [00:11<00:09, 35586.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  32%|███▏      | 213000/666037 [00:07<00:10, 41503.02 examples/s]Running bart tokenizers on train dataset (num_proc=51):  54%|█████▍    | 360000/666037 [00:11<00:07, 40044.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  52%|█████▏    | 345000/666037 [00:11<00:12, 26305.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):  52%|█████▏    | 344000/666037 [00:11<00:07, 42773.77 examples/s]Running bart tokenizers on train dataset (num_proc=51):  58%|█████▊    | 384000/666037 [00:11<00:06, 41623.56 examples/s]Running bart tokenizers on train dataset (num_proc=51):  47%|████▋     | 315000/666037 [00:11<00:09, 36971.40 examples/s]Running bart tokenizers on train dataset (num_proc=51):  51%|█████     | 337000/666037 [00:11<00:06, 47987.54 examples/s]Running bart tokenizers on train dataset (num_proc=51):  55%|█████▍    | 365000/666037 [00:11<00:07, 41208.65 examples/s]Running bart tokenizers on train dataset (num_proc=51):  55%|█████▌    | 369000/666037 [00:11<00:09, 31263.89 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 218000/666037 [00:08<00:11, 38189.40 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 350000/666037 [00:11<00:10, 29578.87 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▊    | 391000/666037 [00:11<00:05, 46972.26 examples/s]Running bart tokenizers on train dataset (num_proc=51):  48%|████▊     | 320000/666037 [00:11<00:08, 39134.98 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▌    | 374000/666037 [00:11<00:08, 35122.65 examples/s]Running bart tokenizers on train dataset (num_proc=51):  52%|█████▏    | 344000/666037 [00:11<00:06, 47621.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 355000/666037 [00:12<00:09, 33270.97 examples/s]Running bart tokenizers on train dataset (num_proc=51):  52%|█████▏    | 349000/666037 [00:11<00:08, 36926.69 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▌    | 370000/666037 [00:11<00:07, 39752.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  33%|███▎      | 223000/666037 [00:08<00:11, 39141.06 examples/s]Running bart tokenizers on train dataset (num_proc=51):  57%|█████▋    | 381000/666037 [00:12<00:06, 42425.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▋    | 375000/666037 [00:12<00:07, 41552.35 examples/s]Running bart tokenizers on train dataset (num_proc=51):  54%|█████▍    | 360000/666037 [00:12<00:08, 35596.00 examples/s]Running bart tokenizers on train dataset (num_proc=51):  49%|████▉     | 325000/666037 [00:11<00:09, 34825.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  34%|███▍      | 229000/666037 [00:08<00:10, 43536.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 354000/666037 [00:11<00:08, 37939.13 examples/s]Running bart tokenizers on train dataset (num_proc=51):  60%|█████▉    | 397000/666037 [00:12<00:06, 39447.97 examples/s]Running bart tokenizers on train dataset (num_proc=51):  58%|█████▊    | 387000/666037 [00:12<00:06, 45010.14 examples/s]Running bart tokenizers on train dataset (num_proc=51):  54%|█████▍    | 360000/666037 [00:12<00:07, 42430.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):  55%|█████▍    | 365000/666037 [00:12<00:08, 36497.41 examples/s]Running bart tokenizers on train dataset (num_proc=51):  57%|█████▋    | 380000/666037 [00:12<00:07, 36632.89 examples/s]Running bart tokenizers on train dataset (num_proc=51):  49%|████▉     | 329000/666037 [00:11<00:11, 29635.75 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████    | 403000/666037 [00:12<00:07, 37147.28 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▉    | 392000/666037 [00:12<00:06, 42105.15 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▌    | 372000/666037 [00:12<00:06, 43161.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 350000/666037 [00:12<00:10, 30922.35 examples/s]Running bart tokenizers on train dataset (num_proc=51):  55%|█████▍    | 366000/666037 [00:12<00:07, 42813.85 examples/s]Running bart tokenizers on train dataset (num_proc=51):  35%|███▌      | 234000/666037 [00:08<00:13, 32526.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  50%|████▉     | 333000/666037 [00:12<00:10, 30473.38 examples/s]Running bart tokenizers on train dataset (num_proc=51):  58%|█████▊    | 384000/666037 [00:12<00:08, 31437.20 examples/s]Running bart tokenizers on train dataset (num_proc=51):  57%|█████▋    | 379000/666037 [00:12<00:06, 46936.00 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████▏   | 408000/666037 [00:12<00:07, 35799.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  60%|█████▉    | 397000/666037 [00:12<00:07, 37701.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▌      | 238000/666037 [00:08<00:14, 30459.89 examples/s]Running bart tokenizers on train dataset (num_proc=51):  51%|█████     | 337000/666037 [00:12<00:11, 29490.84 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▌    | 371000/666037 [00:12<00:08, 34816.40 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▊    | 391000/666037 [00:12<00:07, 36683.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  60%|██████    | 401000/666037 [00:12<00:07, 37575.45 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 355000/666037 [00:12<00:11, 27321.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 412000/666037 [00:12<00:07, 32746.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  58%|█████▊    | 386000/666037 [00:12<00:06, 44155.89 examples/s]Running bart tokenizers on train dataset (num_proc=51):  52%|█████▏    | 345000/666037 [00:12<00:08, 39265.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  54%|█████▍    | 362000/666037 [00:12<00:09, 33696.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▋    | 375000/666037 [00:12<00:08, 32466.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████    | 405000/666037 [00:12<00:07, 35664.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▉    | 395000/666037 [00:12<00:08, 33031.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 416000/666037 [00:12<00:07, 31510.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):  36%|███▋      | 242000/666037 [00:09<00:18, 23096.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▌    | 370000/666037 [00:12<00:07, 41784.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 410000/666037 [00:12<00:06, 39031.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  57%|█████▋    | 380000/666037 [00:12<00:08, 34142.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  60%|██████    | 402000/666037 [00:12<00:06, 40738.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 350000/666037 [00:12<00:09, 33930.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▊    | 391000/666037 [00:12<00:08, 33611.73 examples/s]Running bart tokenizers on train dataset (num_proc=51):  63%|██████▎   | 420000/666037 [00:12<00:07, 31162.33 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 245000/666037 [00:09<00:17, 24217.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 415000/666037 [00:12<00:06, 40920.14 examples/s]Running bart tokenizers on train dataset (num_proc=51):  58%|█████▊    | 385000/666037 [00:12<00:07, 37645.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▋    | 376000/666037 [00:12<00:07, 40608.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  54%|█████▍    | 358000/666037 [00:12<00:07, 41985.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████    | 407000/666037 [00:13<00:06, 38083.21 examples/s]Running bart tokenizers on train dataset (num_proc=51):  64%|██████▍   | 426000/666037 [00:13<00:07, 33894.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  37%|███▋      | 248000/666037 [00:09<00:18, 22854.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  57%|█████▋    | 381000/666037 [00:12<00:06, 41102.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  58%|█████▊    | 389000/666037 [00:12<00:08, 32869.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  63%|██████▎   | 420000/666037 [00:13<00:06, 35754.04 examples/s]Running bart tokenizers on train dataset (num_proc=51):  55%|█████▍    | 363000/666037 [00:12<00:07, 41217.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▉    | 395000/666037 [00:13<00:09, 27244.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 412000/666037 [00:13<00:06, 37560.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  65%|██████▌   | 435000/666037 [00:13<00:05, 44709.62 examples/s]Running bart tokenizers on train dataset (num_proc=51):  58%|█████▊    | 387000/666037 [00:13<00:06, 44300.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 251000/666037 [00:09<00:19, 21696.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  55%|█████▌    | 368000/666037 [00:12<00:06, 43222.56 examples/s]Running bart tokenizers on train dataset (num_proc=51):  64%|██████▍   | 425000/666037 [00:13<00:06, 38048.11 examples/s]Running bart tokenizers on train dataset (num_proc=51):  60%|█████▉    | 399000/666037 [00:13<00:09, 29236.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▉    | 395000/666037 [00:13<00:07, 35621.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 416000/666037 [00:13<00:06, 36677.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  67%|██████▋   | 445000/666037 [00:13<00:04, 54427.94 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████    | 403000/666037 [00:13<00:08, 30860.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▉    | 392000/666037 [00:13<00:06, 40187.23 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▌    | 373000/666037 [00:13<00:07, 41063.18 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████    | 403000/666037 [00:13<00:06, 43760.98 examples/s]Running bart tokenizers on train dataset (num_proc=51):  64%|██████▍   | 429000/666037 [00:13<00:06, 34289.05 examples/s]Running bart tokenizers on train dataset (num_proc=51):  68%|██████▊   | 451000/666037 [00:13<00:03, 55595.30 examples/s]Running bart tokenizers on train dataset (num_proc=51):  63%|██████▎   | 421000/666037 [00:13<00:07, 33122.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████    | 407000/666037 [00:13<00:08, 32100.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  60%|█████▉    | 399000/666037 [00:13<00:05, 45984.29 examples/s]Running bart tokenizers on train dataset (num_proc=51):  65%|██████▌   | 434000/666037 [00:13<00:06, 36896.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  57%|█████▋    | 378000/666037 [00:13<00:07, 39330.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████▏   | 408000/666037 [00:13<00:06, 41384.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  64%|██████▍   | 426000/666037 [00:13<00:06, 36837.06 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 415000/666037 [00:13<00:05, 42041.19 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 254000/666037 [00:09<00:28, 14461.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 414000/666037 [00:13<00:05, 45121.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  66%|██████▌   | 440000/666037 [00:13<00:05, 38734.25 examples/s]Running bart tokenizers on train dataset (num_proc=51):  65%|██████▍   | 432000/666037 [00:13<00:05, 41533.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  63%|██████▎   | 420000/666037 [00:13<00:06, 39307.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  58%|█████▊    | 383000/666037 [00:13<00:08, 31895.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  66%|██████▌   | 437000/666037 [00:13<00:05, 39635.98 examples/s]Running bart tokenizers on train dataset (num_proc=51):  69%|██████▊   | 457000/666037 [00:13<00:06, 31315.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  38%|███▊      | 256000/666037 [00:10<00:30, 13397.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  67%|██████▋   | 444000/666037 [00:13<00:07, 30901.85 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████    | 405000/666037 [00:13<00:08, 29714.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  58%|█████▊    | 387000/666037 [00:13<00:08, 31147.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  63%|██████▎   | 420000/666037 [00:13<00:07, 34389.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  64%|██████▍   | 426000/666037 [00:13<00:06, 37172.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 260000/666037 [00:10<00:23, 17580.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  70%|██████▉   | 464000/666037 [00:13<00:05, 35259.62 examples/s]Running bart tokenizers on train dataset (num_proc=51):  66%|██████▋   | 442000/666037 [00:13<00:06, 37180.75 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 410000/666037 [00:13<00:07, 32124.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▉    | 392000/666037 [00:13<00:07, 35123.38 examples/s]Running bart tokenizers on train dataset (num_proc=51):  64%|██████▎   | 424000/666037 [00:13<00:06, 35188.45 examples/s]Running bart tokenizers on train dataset (num_proc=51):  67%|██████▋   | 449000/666037 [00:14<00:06, 31355.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  39%|███▉      | 263000/666037 [00:10<00:22, 18226.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  67%|██████▋   | 446000/666037 [00:14<00:06, 36503.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  68%|██████▊   | 455000/666037 [00:14<00:05, 37231.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▉    | 396000/666037 [00:13<00:08, 33014.40 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 416000/666037 [00:14<00:07, 33970.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  70%|███████   | 469000/666037 [00:14<00:05, 33468.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  64%|██████▍   | 428000/666037 [00:13<00:07, 33405.57 examples/s]Running bart tokenizers on train dataset (num_proc=51):  65%|██████▍   | 430000/666037 [00:14<00:08, 29147.13 examples/s]Running bart tokenizers on train dataset (num_proc=51):  60%|██████    | 401000/666037 [00:13<00:07, 36910.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):  68%|██████▊   | 450000/666037 [00:14<00:06, 32955.73 examples/s]Running bart tokenizers on train dataset (num_proc=51):  69%|██████▉   | 460000/666037 [00:14<00:05, 37442.25 examples/s]Running bart tokenizers on train dataset (num_proc=51):  65%|██████▌   | 435000/666037 [00:14<00:05, 41418.16 examples/s]Running bart tokenizers on train dataset (num_proc=51):  66%|██████▌   | 438000/666037 [00:14<00:06, 37786.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  71%|███████▏  | 476000/666037 [00:14<00:05, 37547.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  63%|██████▎   | 422000/666037 [00:14<00:07, 34143.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████    | 406000/666037 [00:13<00:06, 40187.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|███▉      | 266000/666037 [00:10<00:25, 15679.83 examples/s]Running bart tokenizers on train dataset (num_proc=51):  68%|██████▊   | 455000/666037 [00:14<00:05, 36678.55 examples/s]Running bart tokenizers on train dataset (num_proc=51):  66%|██████▌   | 440000/666037 [00:14<00:05, 41332.64 examples/s]Running bart tokenizers on train dataset (num_proc=51):  72%|███████▏  | 482000/666037 [00:14<00:04, 41780.28 examples/s]Running bart tokenizers on train dataset (num_proc=51):  70%|██████▉   | 466000/666037 [00:14<00:05, 38657.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  64%|██████▍   | 427000/666037 [00:14<00:06, 36983.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 415000/666037 [00:14<00:04, 51252.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  40%|████      | 268000/666037 [00:10<00:25, 15691.87 examples/s]Running bart tokenizers on train dataset (num_proc=51):  69%|██████▉   | 460000/666037 [00:14<00:05, 37187.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  67%|██████▋   | 443000/666037 [00:14<00:06, 32233.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  71%|███████   | 473000/666037 [00:14<00:04, 43516.35 examples/s]Running bart tokenizers on train dataset (num_proc=51):  73%|███████▎  | 488000/666037 [00:14<00:04, 41124.10 examples/s]Running bart tokenizers on train dataset (num_proc=51):  67%|██████▋   | 445000/666037 [00:14<00:06, 35836.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  65%|██████▌   | 433000/666037 [00:14<00:06, 37295.65 examples/s]Running bart tokenizers on train dataset (num_proc=51):  70%|██████▉   | 464000/666037 [00:14<00:05, 37560.40 examples/s]Running bart tokenizers on train dataset (num_proc=51):  63%|██████▎   | 421000/666037 [00:14<00:05, 45623.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  67%|██████▋   | 448000/666037 [00:14<00:06, 33726.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  72%|███████▏  | 481000/666037 [00:14<00:03, 51699.94 examples/s]Running bart tokenizers on train dataset (num_proc=51):  68%|██████▊   | 452000/666037 [00:14<00:04, 43365.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):  66%|██████▌   | 440000/666037 [00:14<00:05, 43518.74 examples/s]Running bart tokenizers on train dataset (num_proc=51):  74%|███████▍  | 496000/666037 [00:14<00:03, 45560.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 270000/666037 [00:10<00:30, 13161.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  69%|██████▊   | 457000/666037 [00:14<00:05, 36108.69 examples/s]Running bart tokenizers on train dataset (num_proc=51):  70%|███████   | 468000/666037 [00:14<00:07, 25763.64 examples/s]Running bart tokenizers on train dataset (num_proc=51):  64%|██████▍   | 426000/666037 [00:14<00:07, 34084.40 examples/s]Running bart tokenizers on train dataset (num_proc=51):  68%|██████▊   | 452000/666037 [00:14<00:07, 26997.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  73%|███████▎  | 487000/666037 [00:14<00:04, 39513.95 examples/s]Running bart tokenizers on train dataset (num_proc=51):  67%|██████▋   | 445000/666037 [00:14<00:05, 37220.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  75%|███████▌  | 501000/666037 [00:14<00:04, 38854.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 272000/666037 [00:11<00:32, 11966.92 examples/s]Running bart tokenizers on train dataset (num_proc=51):  70%|██████▉   | 464000/666037 [00:14<00:04, 43845.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  65%|██████▌   | 433000/666037 [00:14<00:05, 39894.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  68%|██████▊   | 451000/666037 [00:14<00:05, 39707.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):  72%|███████▏  | 479000/666037 [00:14<00:04, 39493.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  41%|████      | 274000/666037 [00:11<00:29, 13289.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▌  | 506000/666037 [00:15<00:04, 36888.37 examples/s]Running bart tokenizers on train dataset (num_proc=51):  70%|██████▉   | 466000/666037 [00:14<00:04, 42170.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  74%|███████▍  | 493000/666037 [00:15<00:04, 37590.92 examples/s]Running bart tokenizers on train dataset (num_proc=51):  42%|████▏     | 281000/666037 [00:11<00:16, 23790.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 511000/666037 [00:15<00:04, 38223.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  75%|███████▍  | 498000/666037 [00:15<00:04, 40046.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  68%|██████▊   | 456000/666037 [00:15<00:05, 35572.54 examples/s]Running bart tokenizers on train dataset (num_proc=51):  71%|███████   | 470000/666037 [00:15<00:05, 38154.62 examples/s]Running bart tokenizers on train dataset (num_proc=51):  73%|███████▎  | 484000/666037 [00:15<00:05, 34524.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  43%|████▎     | 285000/666037 [00:11<00:14, 27023.70 examples/s]Running bart tokenizers on train dataset (num_proc=51):  71%|███████   | 471000/666037 [00:15<00:05, 34352.50 examples/s]Running bart tokenizers on train dataset (num_proc=51):  70%|██████▉   | 464000/666037 [00:15<00:04, 44870.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  66%|██████▌   | 438000/666037 [00:14<00:07, 28543.57 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 516000/666037 [00:15<00:04, 35502.56 examples/s]Running bart tokenizers on train dataset (num_proc=51):  74%|███████▎  | 490000/666037 [00:15<00:04, 37701.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  72%|███████▏  | 477000/666037 [00:15<00:04, 38716.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  43%|████▎     | 289000/666037 [00:11<00:13, 28663.63 examples/s]Running bart tokenizers on train dataset (num_proc=51):  67%|██████▋   | 445000/666037 [00:15<00:06, 35637.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  71%|███████▏  | 475000/666037 [00:15<00:06, 31205.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▌  | 503000/666037 [00:15<00:05, 30070.62 examples/s]Running bart tokenizers on train dataset (num_proc=51):  78%|███████▊  | 520000/666037 [00:15<00:04, 34461.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  74%|███████▍  | 495000/666037 [00:15<00:04, 36056.43 examples/s]Running bart tokenizers on train dataset (num_proc=51):  44%|████▍     | 293000/666037 [00:11<00:12, 29210.42 examples/s]Running bart tokenizers on train dataset (num_proc=51):  68%|██████▊   | 452000/666037 [00:15<00:05, 41433.23 examples/s]Running bart tokenizers on train dataset (num_proc=51):  71%|███████   | 470000/666037 [00:15<00:05, 36506.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):  72%|███████▏  | 481000/666037 [00:15<00:05, 36756.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▌  | 507000/666037 [00:15<00:05, 30639.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  79%|███████▉  | 526000/666037 [00:15<00:03, 38459.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  75%|███████▌  | 501000/666037 [00:15<00:04, 38649.20 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▍     | 299000/666037 [00:11<00:10, 34435.20 examples/s]Running bart tokenizers on train dataset (num_proc=51):  72%|███████▏  | 482000/666037 [00:15<00:05, 31036.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  73%|███████▎  | 486000/666037 [00:15<00:04, 39411.85 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 515000/666037 [00:15<00:03, 38924.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):  69%|██████▉   | 458000/666037 [00:15<00:05, 39800.87 examples/s]Running bart tokenizers on train dataset (num_proc=51):  71%|███████▏  | 475000/666037 [00:15<00:05, 34589.74 examples/s]Running bart tokenizers on train dataset (num_proc=51):  80%|███████▉  | 531000/666037 [00:15<00:03, 38181.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  74%|███████▍  | 494000/666037 [00:15<00:03, 48861.11 examples/s]Running bart tokenizers on train dataset (num_proc=51):  73%|███████▎  | 487000/666037 [00:15<00:05, 33767.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▌  | 506000/666037 [00:15<00:04, 39365.45 examples/s]Running bart tokenizers on train dataset (num_proc=51):  79%|███████▊  | 524000/666037 [00:15<00:02, 49687.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):  70%|██████▉   | 464000/666037 [00:15<00:04, 42425.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  45%|████▌     | 303000/666037 [00:12<00:12, 28870.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  72%|███████▏  | 479000/666037 [00:15<00:05, 32647.07 examples/s]Running bart tokenizers on train dataset (num_proc=51):  80%|████████  | 535000/666037 [00:15<00:03, 34521.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  74%|███████▍  | 493000/666037 [00:15<00:04, 37918.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  75%|███████▌  | 500000/666037 [00:15<00:03, 44948.21 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 511000/666037 [00:15<00:04, 33962.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):  73%|███████▎  | 484000/666037 [00:15<00:05, 35388.30 examples/s]Running bart tokenizers on train dataset (num_proc=51):  47%|████▋     | 313000/666037 [00:12<00:08, 41352.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  80%|███████▉  | 530000/666037 [00:15<00:03, 42917.78 examples/s]Running bart tokenizers on train dataset (num_proc=51):  81%|████████▏ | 542000/666037 [00:15<00:03, 37976.11 examples/s]Running bart tokenizers on train dataset (num_proc=51):  71%|███████   | 473000/666037 [00:15<00:04, 42394.92 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▌  | 505000/666037 [00:15<00:03, 43462.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 516000/666037 [00:16<00:04, 37151.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  73%|███████▎  | 488000/666037 [00:15<00:04, 36244.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  75%|███████▍  | 498000/666037 [00:16<00:05, 33317.14 examples/s]Running bart tokenizers on train dataset (num_proc=51):  80%|████████  | 535000/666037 [00:16<00:03, 40459.63 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 510000/666037 [00:15<00:03, 44378.53 examples/s]Running bart tokenizers on train dataset (num_proc=51):  72%|███████▏  | 478000/666037 [00:15<00:04, 42794.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  78%|███████▊  | 521000/666037 [00:16<00:03, 39591.85 examples/s]Running bart tokenizers on train dataset (num_proc=51):  74%|███████▍  | 492000/666037 [00:16<00:04, 36137.40 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▌  | 504000/666037 [00:16<00:04, 37000.06 examples/s]Running bart tokenizers on train dataset (num_proc=51):  82%|████████▏ | 548000/666037 [00:16<00:03, 34515.89 examples/s]Running bart tokenizers on train dataset (num_proc=51):  81%|████████  | 541000/666037 [00:16<00:02, 44479.30 examples/s]Running bart tokenizers on train dataset (num_proc=51):  73%|███████▎  | 485000/666037 [00:15<00:03, 47937.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 515000/666037 [00:16<00:03, 42773.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):  79%|███████▉  | 529000/666037 [00:16<00:02, 48808.59 examples/s]Running bart tokenizers on train dataset (num_proc=51):  75%|███████▍  | 498000/666037 [00:16<00:04, 41154.91 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 512000/666037 [00:16<00:03, 44449.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  48%|████▊     | 318000/666037 [00:12<00:12, 28248.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  83%|████████▎ | 556000/666037 [00:16<00:02, 39451.76 examples/s]Running bart tokenizers on train dataset (num_proc=51):  82%|████████▏ | 546060/666037 [00:16<00:03, 39275.59 examples/s]Running bart tokenizers on train dataset (num_proc=51):  49%|████▊     | 324000/666037 [00:12<00:10, 32835.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▌  | 503000/666037 [00:16<00:04, 39323.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  78%|███████▊  | 517000/666037 [00:16<00:03, 41951.63 examples/s]Running bart tokenizers on train dataset (num_proc=51):  80%|████████  | 535000/666037 [00:16<00:03, 41796.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  84%|████████▍ | 561000/666037 [00:16<00:02, 39108.59 examples/s]Running bart tokenizers on train dataset (num_proc=51):  49%|████▉     | 329000/666037 [00:12<00:09, 36054.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  78%|███████▊  | 520000/666037 [00:16<00:04, 31134.14 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▋  | 509000/666037 [00:16<00:03, 42898.57 examples/s]Running bart tokenizers on train dataset (num_proc=51):  85%|████████▍ | 565000/666037 [00:16<00:02, 38965.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  83%|████████▎ | 551060/666037 [00:16<00:03, 32584.61 examples/s]Running bart tokenizers on train dataset (num_proc=51):  78%|███████▊  | 522000/666037 [00:16<00:03, 36595.09 examples/s]Running bart tokenizers on train dataset (num_proc=51):  74%|███████▎  | 491000/666037 [00:16<00:05, 30102.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  50%|█████     | 334000/666037 [00:12<00:08, 37022.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 514000/666037 [00:16<00:03, 42270.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):  79%|███████▊  | 524000/666037 [00:16<00:04, 28596.27 examples/s]Running bart tokenizers on train dataset (num_proc=51):  85%|████████▌ | 569000/666037 [00:16<00:02, 37572.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  79%|███████▉  | 526000/666037 [00:16<00:03, 37147.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  75%|███████▍  | 498000/666037 [00:16<00:04, 36559.75 examples/s]Running bart tokenizers on train dataset (num_proc=51):  81%|████████  | 540000/666037 [00:16<00:04, 30183.18 examples/s]Running bart tokenizers on train dataset (num_proc=51):  83%|████████▎ | 556120/666037 [00:16<00:03, 33896.65 examples/s]Running bart tokenizers on train dataset (num_proc=51):  78%|███████▊  | 519000/666037 [00:16<00:03, 40415.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  51%|█████     | 339000/666037 [00:13<00:09, 35244.83 examples/s]Running bart tokenizers on train dataset (num_proc=51):  80%|███████▉  | 530000/666037 [00:16<00:04, 32984.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▌  | 504000/666037 [00:16<00:04, 39751.43 examples/s]Running bart tokenizers on train dataset (num_proc=51):  86%|████████▌ | 574000/666037 [00:16<00:02, 37042.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  82%|████████▏ | 546060/666037 [00:16<00:03, 34927.20 examples/s]Running bart tokenizers on train dataset (num_proc=51):  80%|███████▉  | 530000/666037 [00:16<00:04, 31882.43 examples/s]Running bart tokenizers on train dataset (num_proc=51):  84%|████████▍ | 561239/666037 [00:16<00:03, 32815.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  81%|████████  | 538000/666037 [00:16<00:03, 42029.00 examples/s]Running bart tokenizers on train dataset (num_proc=51):  51%|█████▏    | 343000/666037 [00:13<00:09, 32947.85 examples/s]Running bart tokenizers on train dataset (num_proc=51):  79%|███████▊  | 524000/666037 [00:16<00:03, 37299.23 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▋  | 509000/666037 [00:16<00:03, 40135.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  87%|████████▋ | 578060/666037 [00:16<00:02, 35248.13 examples/s]Running bart tokenizers on train dataset (num_proc=51):  83%|████████▎ | 551060/666037 [00:16<00:03, 34901.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  80%|████████  | 534060/666037 [00:17<00:04, 31021.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  85%|████████▌ | 567239/666037 [00:17<00:02, 36392.29 examples/s]Running bart tokenizers on train dataset (num_proc=51):  82%|████████▏ | 544000/666037 [00:16<00:02, 45650.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):  52%|█████▏    | 348000/666037 [00:13<00:09, 35054.23 examples/s]Running bart tokenizers on train dataset (num_proc=51):  79%|███████▉  | 528000/666037 [00:16<00:03, 34733.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):  88%|████████▊ | 584120/666037 [00:17<00:02, 38481.58 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 515000/666037 [00:16<00:03, 39303.56 examples/s]Running bart tokenizers on train dataset (num_proc=51):  81%|████████  | 538060/666037 [00:17<00:03, 32727.29 examples/s]Running bart tokenizers on train dataset (num_proc=51):  53%|█████▎    | 353000/666037 [00:13<00:08, 38436.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  86%|████████▌ | 572239/666037 [00:17<00:02, 37318.18 examples/s]Running bart tokenizers on train dataset (num_proc=51):  83%|████████▎ | 555180/666037 [00:17<00:03, 30702.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  83%|████████▎ | 554060/666037 [00:17<00:02, 53467.92 examples/s]Running bart tokenizers on train dataset (num_proc=51):  78%|███████▊  | 520000/666037 [00:16<00:03, 40406.23 examples/s]Running bart tokenizers on train dataset (num_proc=51):  81%|████████▏ | 542060/666037 [00:17<00:03, 33830.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  80%|███████▉  | 532000/666037 [00:17<00:04, 30831.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  88%|████████▊ | 588180/666037 [00:17<00:02, 34973.19 examples/s]Running bart tokenizers on train dataset (num_proc=51):  84%|████████▍ | 561180/666037 [00:17<00:02, 36022.38 examples/s]Running bart tokenizers on train dataset (num_proc=51):  87%|████████▋ | 580358/666037 [00:17<00:01, 44590.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  54%|█████▍    | 362000/666037 [00:13<00:06, 45841.26 examples/s]Running bart tokenizers on train dataset (num_proc=51):  79%|███████▉  | 527000/666037 [00:17<00:03, 46141.18 examples/s]Running bart tokenizers on train dataset (num_proc=51):  82%|████████▏ | 548060/666037 [00:17<00:03, 38802.29 examples/s]Running bart tokenizers on train dataset (num_proc=51):  89%|████████▉ | 593360/666037 [00:17<00:01, 37599.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  81%|████████  | 539000/666037 [00:17<00:03, 37153.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  88%|████████▊ | 586477/666037 [00:17<00:01, 48093.94 examples/s]Running bart tokenizers on train dataset (num_proc=51):  85%|████████▌ | 568180/666037 [00:17<00:02, 41925.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  84%|████████▍ | 560060/666037 [00:17<00:02, 39481.33 examples/s]Running bart tokenizers on train dataset (num_proc=51):  83%|████████▎ | 553179/666037 [00:17<00:02, 41198.65 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|████████▉ | 598360/666037 [00:17<00:01, 39933.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  80%|███████▉  | 532000/666037 [00:17<00:03, 42731.09 examples/s]Running bart tokenizers on train dataset (num_proc=51):  82%|████████▏ | 543000/666037 [00:17<00:03, 35855.88 examples/s]Running bart tokenizers on train dataset (num_proc=51):  86%|████████▌ | 573180/666037 [00:17<00:02, 43316.29 examples/s]Running bart tokenizers on train dataset (num_proc=51):  89%|████████▉ | 591596/666037 [00:17<00:01, 41204.38 examples/s]Running bart tokenizers on train dataset (num_proc=51):  55%|█████▌    | 368000/666037 [00:13<00:08, 35258.82 examples/s]Running bart tokenizers on train dataset (num_proc=51):  85%|████████▌ | 568120/666037 [00:17<00:02, 44670.74 examples/s]Running bart tokenizers on train dataset (num_proc=51):  84%|████████▍ | 559239/666037 [00:17<00:02, 44023.06 examples/s]Running bart tokenizers on train dataset (num_proc=51):  91%|█████████ | 603419/666037 [00:17<00:01, 41540.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  81%|████████  | 538000/666037 [00:17<00:02, 46596.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  82%|████████▏ | 548000/666037 [00:17<00:03, 37371.45 examples/s]Running bart tokenizers on train dataset (num_proc=51):  87%|████████▋ | 581299/666037 [00:17<00:01, 46211.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|████████▉ | 596775/666037 [00:17<00:01, 43623.56 examples/s]Running bart tokenizers on train dataset (num_proc=51):  91%|█████████▏| 608539/666037 [00:17<00:01, 43397.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  85%|████████▌ | 567298/666037 [00:17<00:01, 52260.67 examples/s]Running bart tokenizers on train dataset (num_proc=51):  56%|█████▌    | 372000/666037 [00:13<00:08, 34080.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  82%|████████▏ | 543000/666037 [00:17<00:02, 42025.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  84%|████████▎ | 557000/666037 [00:17<00:02, 45390.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  88%|████████▊ | 586419/666037 [00:17<00:01, 43646.82 examples/s]Running bart tokenizers on train dataset (num_proc=51):  86%|████████▌ | 573180/666037 [00:17<00:02, 36267.13 examples/s]Running bart tokenizers on train dataset (num_proc=51):  92%|█████████▏| 613837/666037 [00:17<00:01, 40263.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  57%|█████▋    | 379000/666037 [00:14<00:07, 37814.33 examples/s]Running bart tokenizers on train dataset (num_proc=51):  84%|████████▍ | 562000/666037 [00:17<00:02, 42832.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  89%|████████▉ | 591479/666037 [00:17<00:01, 44845.64 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|█████████ | 601893/666037 [00:17<00:01, 32767.18 examples/s]Running bart tokenizers on train dataset (num_proc=51):  82%|████████▏ | 548000/666037 [00:17<00:03, 34878.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  93%|█████████▎| 620016/666037 [00:17<00:01, 45269.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  87%|████████▋ | 578180/666037 [00:17<00:02, 35608.05 examples/s]Running bart tokenizers on train dataset (num_proc=51):  86%|████████▌ | 573357/666037 [00:17<00:02, 38573.25 examples/s]Running bart tokenizers on train dataset (num_proc=51):  58%|█████▊    | 385000/666037 [00:14<00:06, 40644.18 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|████████▉ | 596598/666037 [00:18<00:01, 43133.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  91%|█████████ | 605953/666037 [00:18<00:01, 33305.01 examples/s]Running bart tokenizers on train dataset (num_proc=51):  88%|████████▊ | 583180/666037 [00:17<00:02, 37992.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  85%|████████▌ | 567000/666037 [00:17<00:02, 38653.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  59%|█████▊    | 391000/666037 [00:14<00:06, 44024.25 examples/s]Running bart tokenizers on train dataset (num_proc=51):  83%|████████▎ | 554000/666037 [00:17<00:02, 37560.00 examples/s]Running bart tokenizers on train dataset (num_proc=51):  94%|█████████▍| 625134/666037 [00:18<00:00, 43147.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  87%|████████▋ | 581357/666037 [00:18<00:02, 41981.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|█████████ | 601657/666037 [00:18<00:01, 42951.92 examples/s]Running bart tokenizers on train dataset (num_proc=51):  88%|████████▊ | 588240/666037 [00:18<00:02, 38450.22 examples/s]Running bart tokenizers on train dataset (num_proc=51):  86%|████████▌ | 571000/666037 [00:18<00:02, 35901.12 examples/s]Running bart tokenizers on train dataset (num_proc=51):  60%|██████    | 400000/666037 [00:14<00:05, 51306.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  92%|█████████▏| 610133/666037 [00:18<00:01, 29048.24 examples/s]Running bart tokenizers on train dataset (num_proc=51):  95%|█████████▍| 630372/666037 [00:18<00:00, 36601.15 examples/s]Running bart tokenizers on train dataset (num_proc=51):  84%|████████▍ | 561060/666037 [00:17<00:02, 36728.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  89%|████████▉ | 593240/666037 [00:18<00:01, 40954.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  86%|████████▋ | 576000/666037 [00:18<00:02, 39004.26 examples/s]Running bart tokenizers on train dataset (num_proc=51):  88%|████████▊ | 586357/666037 [00:18<00:02, 36127.64 examples/s]Running bart tokenizers on train dataset (num_proc=51):  91%|█████████ | 606835/666037 [00:18<00:01, 38240.50 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|█████████ | 600240/666037 [00:18<00:01, 47659.48 examples/s]Running bart tokenizers on train dataset (num_proc=51):  92%|█████████▏| 614193/666037 [00:18<00:01, 27999.60 examples/s]Running bart tokenizers on train dataset (num_proc=51):  61%|██████▏   | 408000/666037 [00:14<00:05, 48032.70 examples/s]Running bart tokenizers on train dataset (num_proc=51):  87%|████████▋ | 580059/666037 [00:18<00:02, 39005.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  85%|████████▌ | 568060/666037 [00:18<00:02, 40999.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  89%|████████▉ | 591417/666037 [00:18<00:01, 38840.86 examples/s]Running bart tokenizers on train dataset (num_proc=51):  92%|█████████▏| 612953/666037 [00:18<00:01, 43173.76 examples/s]Running bart tokenizers on train dataset (num_proc=51):  95%|█████████▌| 634551/666037 [00:18<00:00, 31788.26 examples/s]Running bart tokenizers on train dataset (num_proc=51):  93%|█████████▎| 621313/666037 [00:18<00:01, 35914.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  91%|█████████ | 605359/666037 [00:18<00:01, 45461.82 examples/s]Running bart tokenizers on train dataset (num_proc=51):  86%|████████▌ | 574060/666037 [00:18<00:02, 44766.33 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|████████▉ | 596536/666037 [00:18<00:01, 41096.96 examples/s]Running bart tokenizers on train dataset (num_proc=51):  62%|██████▏   | 413000/666037 [00:14<00:05, 44347.09 examples/s]Running bart tokenizers on train dataset (num_proc=51):  93%|█████████▎| 618132/666037 [00:18<00:01, 40980.59 examples/s]Running bart tokenizers on train dataset (num_proc=51):  88%|████████▊ | 584119/666037 [00:18<00:02, 31773.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  96%|█████████▌| 638670/666037 [00:18<00:00, 30622.28 examples/s]Running bart tokenizers on train dataset (num_proc=51):  94%|█████████▍| 628432/666037 [00:18<00:00, 43267.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|█████████ | 602655/666037 [00:18<00:01, 45870.25 examples/s]Running bart tokenizers on train dataset (num_proc=51):  92%|█████████▏| 612538/666037 [00:18<00:01, 49510.56 examples/s]Running bart tokenizers on train dataset (num_proc=51):  87%|████████▋ | 579120/666037 [00:18<00:01, 43581.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  63%|██████▎   | 418000/666037 [00:14<00:05, 43113.80 examples/s]Running bart tokenizers on train dataset (num_proc=51):  94%|█████████▎| 623191/666037 [00:18<00:00, 42919.02 examples/s]Running bart tokenizers on train dataset (num_proc=51):  89%|████████▉ | 591238/666037 [00:18<00:01, 40008.02 examples/s]Running bart tokenizers on train dataset (num_proc=51):  97%|█████████▋| 644790/666037 [00:18<00:00, 36406.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  92%|█████████▏| 610714/666037 [00:18<00:01, 50268.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  88%|████████▊ | 586120/666037 [00:18<00:01, 47767.04 examples/s]Running bart tokenizers on train dataset (num_proc=51):  95%|█████████▌| 633552/666037 [00:18<00:00, 39091.30 examples/s]Running bart tokenizers on train dataset (num_proc=51):  64%|██████▍   | 429000/666037 [00:15<00:04, 56157.70 examples/s]Running bart tokenizers on train dataset (num_proc=51):  95%|█████████▍| 632429/666037 [00:18<00:00, 54028.49 examples/s]Running bart tokenizers on train dataset (num_proc=51):  93%|█████████▎| 617838/666037 [00:18<00:01, 43533.83 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|████████▉ | 599238/666037 [00:18<00:01, 48848.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  97%|█████████▋| 648969/666037 [00:18<00:00, 37626.50 examples/s]Running bart tokenizers on train dataset (num_proc=51):  93%|█████████▎| 616893/666037 [00:18<00:00, 51560.31 examples/s]Running bart tokenizers on train dataset (num_proc=51):  89%|████████▉ | 591300/666037 [00:18<00:01, 45641.72 examples/s]Running bart tokenizers on train dataset (num_proc=51):  66%|██████▌   | 438000/666037 [00:15<00:03, 62981.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  96%|█████████▌| 638488/666037 [00:18<00:00, 53099.30 examples/s]Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 653266/666037 [00:18<00:00, 38660.05 examples/s]Running bart tokenizers on train dataset (num_proc=51):  91%|█████████ | 605358/666037 [00:18<00:01, 49445.47 examples/s]Running bart tokenizers on train dataset (num_proc=51):  94%|█████████▍| 625191/666037 [00:18<00:00, 58170.82 examples/s]Running bart tokenizers on train dataset (num_proc=51):  94%|█████████▎| 623018/666037 [00:18<00:01, 38343.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|█████████ | 600420/666037 [00:18<00:01, 54810.81 examples/s]Running bart tokenizers on train dataset (num_proc=51):  67%|██████▋   | 445000/666037 [00:15<00:03, 61246.87 examples/s]Running bart tokenizers on train dataset (num_proc=51):  92%|█████████▏| 613477/666037 [00:18<00:00, 57153.78 examples/s]Running bart tokenizers on train dataset (num_proc=51):  94%|█████████▍| 628257/666037 [00:18<00:00, 39988.45 examples/s]Running bart tokenizers on train dataset (num_proc=51):  68%|██████▊   | 452000/666037 [00:15<00:03, 63278.32 examples/s]Running bart tokenizers on train dataset (num_proc=51):  96%|█████████▌| 638672/666037 [00:19<00:01, 26842.25 examples/s]Running bart tokenizers on train dataset (num_proc=51):  97%|█████████▋| 644667/666037 [00:19<00:00, 42205.42 examples/s]Running bart tokenizers on train dataset (num_proc=51):  91%|█████████ | 606660/666037 [00:18<00:01, 53434.11 examples/s]Running bart tokenizers on train dataset (num_proc=51):  95%|█████████▍| 631309/666037 [00:19<00:00, 51721.42 examples/s]Running bart tokenizers on train dataset (num_proc=51):  93%|█████████▎| 620717/666037 [00:19<00:00, 57519.44 examples/s]Running bart tokenizers on train dataset (num_proc=51):  95%|█████████▌| 633257/666037 [00:19<00:00, 39988.76 examples/s]Running bart tokenizers on train dataset (num_proc=51):  69%|██████▉   | 461000/666037 [00:15<00:02, 69890.71 examples/s]Running bart tokenizers on train dataset (num_proc=51):  99%|█████████▊| 657385/666037 [00:19<00:00, 25346.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  92%|█████████▏| 612720/666037 [00:18<00:00, 53923.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  97%|█████████▋| 643850/666037 [00:19<00:00, 30022.13 examples/s]Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 649785/666037 [00:19<00:00, 39536.39 examples/s]Running bart tokenizers on train dataset (num_proc=51):  94%|█████████▍| 626955/666037 [00:19<00:00, 54394.56 examples/s]Running bart tokenizers on train dataset (num_proc=51):  96%|█████████▌| 638434/666037 [00:19<00:00, 42578.36 examples/s]Running bart tokenizers on train dataset (num_proc=51):  96%|█████████▌| 637428/666037 [00:19<00:00, 43976.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  71%|███████   | 471000/666037 [00:15<00:02, 77491.51 examples/s]Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 652909/666037 [00:19<00:00, 40917.03 examples/s]Running bart tokenizers on train dataset (num_proc=51):  93%|█████████▎| 618780/666037 [00:19<00:00, 50311.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 654785/666037 [00:19<00:00, 40182.63 examples/s]Running bart tokenizers on train dataset (num_proc=51):  97%|█████████▋| 643428/666037 [00:19<00:00, 46842.64 examples/s]Running bart tokenizers on train dataset (num_proc=51):  72%|███████▏  | 481000/666037 [00:15<00:02, 82236.06 examples/s]Running bart tokenizers on train dataset (num_proc=51):  95%|█████████▌| 633192/666037 [00:19<00:00, 51289.99 examples/s]Running bart tokenizers on train dataset (num_proc=51):  97%|█████████▋| 643494/666037 [00:19<00:00, 39582.56 examples/s]Running bart tokenizers on train dataset (num_proc=51):  94%|█████████▍| 624900/666037 [00:19<00:00, 52948.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 649428/666037 [00:19<00:00, 49966.08 examples/s]Running bart tokenizers on train dataset (num_proc=51):  74%|███████▍  | 492000/666037 [00:15<00:01, 87084.14 examples/s]Running bart tokenizers on train dataset (num_proc=51):  96%|█████████▋| 641371/666037 [00:19<00:00, 56656.66 examples/s]Running bart tokenizers on train dataset (num_proc=51):  99%|█████████▉| 661385/666037 [00:19<00:00, 19684.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  99%|█████████▉| 659785/666037 [00:19<00:00, 36668.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  96%|█████████▌| 636079/666037 [00:19<00:00, 68297.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 650674/666037 [00:19<00:00, 45600.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  99%|█████████▉| 658147/666037 [00:19<00:00, 31421.16 examples/s]Running bart tokenizers on train dataset (num_proc=51):  76%|███████▌  | 506000/666037 [00:15<00:01, 101217.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  97%|█████████▋| 648371/666037 [00:19<00:00, 56867.09 examples/s]Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 655428/666037 [00:19<00:00, 46045.79 examples/s]Running bart tokenizers on train dataset (num_proc=51):  99%|█████████▉| 660147/666037 [00:30<00:00, 31421.16 examples/s]Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 655428/666037 [00:30<00:00, 46045.79 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 663385/666037 [00:30<00:00, 19684.93 examples/s]Running bart tokenizers on train dataset (num_proc=51):  99%|█████████▉| 660785/666037 [00:29<00:00, 36668.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  97%|█████████▋| 648371/666037 [00:29<00:00, 56867.09 examples/s]Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 654792/666037 [00:29<00:00, 45600.17 examples/s]Running bart tokenizers on train dataset (num_proc=51):  96%|█████████▋| 642319/666037 [00:29<00:00, 68297.90 examples/s]Running bart tokenizers on train dataset (num_proc=51):  77%|███████▋  | 516000/666037 [00:30<00:01, 101217.46 examples/s]Running bart tokenizers on train dataset (num_proc=51):  97%|█████████▋| 648431/666037 [00:58<00:41, 421.42 examples/s]  Running bart tokenizers on train dataset (num_proc=51):  78%|███████▊  | 517000/666037 [00:54<02:39, 934.13 examples/s]   Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 655487/666037 [00:58<00:28, 370.65 examples/s]  Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 663385/666037 [00:58<00:08, 308.89 examples/s]  Running bart tokenizers on train dataset (num_proc=51):  96%|█████████▋| 642378/666037 [00:58<00:37, 623.49 examples/s]  Running bart tokenizers on train dataset (num_proc=51):  99%|█████████▉| 660147/666037 [00:58<00:14, 414.41 examples/s]  Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 654851/666037 [00:58<00:24, 453.48 examples/s]  Running bart tokenizers on train dataset (num_proc=51):  99%|█████████▉| 660844/666037 [00:58<00:14, 352.19 examples/s]  Running bart tokenizers on train dataset (num_proc=51):  99%|█████████▉| 661496/666037 [00:58<00:03, 1310.68 examples/s]Running bart tokenizers on train dataset (num_proc=51):  85%|████████▌ | 567059/666037 [00:54<00:36, 2741.88 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 664844/666037 [00:58<00:02, 493.63 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 664206/666037 [00:58<00:03, 557.26 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 664911/666037 [00:58<00:01, 812.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  90%|████████▉ | 597059/666037 [00:55<00:16, 4274.23 examples/s]Running bart tokenizers on train dataset (num_proc=51):  92%|█████████▏| 616059/666037 [00:55<00:08, 5625.34 examples/s]Running bart tokenizers on train dataset (num_proc=51):  95%|█████████▌| 635059/666037 [00:55<00:04, 7525.76 examples/s]Running bart tokenizers on train dataset (num_proc=51):  98%|█████████▊| 653059/666037 [00:55<00:01, 9961.27 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665385/666037 [01:06<00:02, 296.75 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 664490/666037 [01:12<00:02, 657.40 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 664727/666037 [01:13<00:02, 459.86 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 664846/666037 [01:14<00:02, 445.84 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 664608/666037 [01:15<00:02, 601.42 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665798/666037 [01:18<00:00, 557.26 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665858/666037 [01:18<00:00, 296.75 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665797/666037 [01:18<00:00, 493.63 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665506/666037 [01:18<00:00, 812.34 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665088/666037 [01:18<00:00, 1310.68 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 663119/666037 [01:14<00:00, 9961.27 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665858/666037 [01:19<00:00, 206.94 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665797/666037 [01:19<00:00, 261.82 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665857/666037 [01:19<00:00, 262.97 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665798/666037 [01:19<00:00, 306.84 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665565/666037 [01:20<00:01, 393.94 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665858/666037 [01:22<00:00, 284.42 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665918/666037 [01:24<00:00, 176.30 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665617/666037 [01:28<00:00, 445.84 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665145/666037 [01:28<00:01, 601.42 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665617/666037 [01:31<00:01, 266.43 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665204/666037 [01:34<00:02, 346.91 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665263/666037 [01:34<00:02, 347.51 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|██████████| 666037/666037 [01:34<00:00, 256.52 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665148/666037 [01:35<00:01, 508.70 examples/s] Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665624/666037 [01:36<00:01, 258.92 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 663178/666037 [01:32<00:02, 1386.52 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665325/666037 [01:36<00:01, 510.80 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 663356/666037 [01:32<00:01, 1387.65 examples/s]Running bart tokenizers on train dataset (num_proc=51): 100%|█████████▉| 665683/666037 [01:36<00:01, 259.09 examples/s]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Running bart tokenizers on validationdataset (num_proc=51):   0%|          | 0/74005 [00:00<?, ? examples/s]Running bart tokenizers on validationdataset (num_proc=51):   1%|▏         | 1000/74005 [00:00<00:36, 2015.80 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  24%|██▎       | 17452/74005 [00:00<00:01, 37846.74 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  58%|█████▊    | 42611/74005 [00:00<00:00, 88969.89 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   0%|          | 0/74005 [00:00<?, ? examples/s]Running bart tokenizers on validationdataset (num_proc=51):   0%|          | 0/74005 [00:00<?, ? examples/s]Running bart tokenizers on validationdataset (num_proc=51):  80%|████████  | 59240/74005 [00:00<00:00, 98909.73 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   0%|          | 0/74005 [00:00<?, ? examples/s]Running bart tokenizers on validationdataset (num_proc=51):   0%|          | 0/74005 [00:00<?, ? examples/s]Running bart tokenizers on validationdataset (num_proc=51):   0%|          | 0/74005 [00:00<?, ? examples/s]Running bart tokenizers on validationdataset (num_proc=51):   0%|          | 0/74005 [00:00<?, ? examples/s]Running bart tokenizers on validationdataset (num_proc=51):   1%|▏         | 1000/74005 [00:00<00:39, 1849.20 examples/s]Running bart tokenizers on validationdataset (num_proc=51): 100%|██████████| 74005/74005 [00:01<00:00, 59033.15 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  11%|█         | 8000/74005 [00:00<00:04, 15954.65 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  20%|██        | 14904/74005 [00:00<00:02, 27930.77 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   1%|▏         | 1000/74005 [00:00<00:49, 1488.09 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   0%|          | 0/74005 [00:00<?, ? examples/s]Running bart tokenizers on validationdataset (num_proc=51):  30%|██▉       | 22161/74005 [00:00<00:01, 38339.15 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   7%|▋         | 5000/74005 [00:00<00:08, 8237.84 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   1%|▏         | 1000/74005 [00:00<00:53, 1368.96 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  42%|████▏     | 30965/74005 [00:00<00:00, 50649.17 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  16%|█▌        | 12000/74005 [00:00<00:03, 19661.06 examples/s]                                                                                                                           Running bart tokenizers on validationdataset (num_proc=51):   4%|▍         | 3000/74005 [00:00<00:16, 4356.53 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  52%|█████▏    | 38220/74005 [00:01<00:00, 55104.38 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  23%|██▎       | 16903/74005 [00:01<00:02, 25201.28 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  61%|██████    | 45122/74005 [00:01<00:00, 58242.36 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   8%|▊         | 6000/74005 [00:00<00:08, 8349.23 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  36%|███▌      | 26805/74005 [00:01<00:01, 40157.00 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  71%|███████   | 52279/74005 [00:01<00:00, 51289.92 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  45%|████▍     | 33160/74005 [00:01<00:00, 44013.80 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  11%|█▏        | 8452/74005 [00:01<00:06, 10231.04 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  79%|███████▉  | 58632/74005 [00:01<00:00, 54138.47 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  53%|█████▎    | 39415/74005 [00:01<00:00, 48075.83 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  15%|█▌        | 11356/74005 [00:01<00:04, 13915.43 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   1%|▏         | 1000/74005 [00:01<01:20, 909.11 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   1%|▏         | 1000/74005 [00:01<01:28, 829.13 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  62%|██████▏   | 45671/74005 [00:01<00:00, 49771.24 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  88%|████████▊ | 64789/74005 [00:01<00:00, 52293.84 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  22%|██▏       | 16356/74005 [00:01<00:02, 21592.26 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   3%|▎         | 2000/74005 [00:01<00:37, 1901.73 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   4%|▍         | 3000/74005 [00:01<00:26, 2704.85 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  71%|███████▏  | 52828/74005 [00:01<00:00, 54964.25 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  28%|██▊       | 20710/74005 [00:01<00:02, 25670.38 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   7%|▋         | 5000/74005 [00:01<00:12, 5444.66 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  12%|█▏        | 9000/74005 [00:01<00:06, 9517.47 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   1%|▏         | 1000/74005 [00:01<01:38, 739.78 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  38%|███▊      | 28161/74005 [00:01<00:01, 37151.17 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  96%|█████████▌| 70848/74005 [00:01<00:00, 39793.68 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  80%|████████  | 59338/74005 [00:01<00:00, 44783.48 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  20%|█▉        | 14452/74005 [00:01<00:03, 15598.11 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   3%|▎         | 2000/74005 [00:01<00:45, 1590.35 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  45%|████▌     | 33514/74005 [00:01<00:01, 39662.57 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  11%|█▏        | 8452/74005 [00:01<00:07, 8740.91 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  87%|████████▋ | 64495/74005 [00:01<00:00, 46112.06 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  26%|██▌       | 18904/74005 [00:01<00:02, 20307.78 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   5%|▌         | 4000/74005 [00:01<00:18, 3725.49 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   1%|▏         | 1000/74005 [00:01<01:28, 822.32 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  52%|█████▏    | 38318/74005 [00:01<00:00, 41286.16 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  18%|█▊        | 13355/74005 [00:01<00:04, 14930.30 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  41%|████      | 30513/74005 [00:01<00:01, 38256.75 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  16%|█▌        | 11904/74005 [00:01<00:04, 14455.86 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  63%|██████▎   | 46573/74005 [00:01<00:00, 52302.98 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  26%|██▌       | 19257/74005 [00:01<00:02, 22946.67 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  94%|█████████▍| 69750/74005 [00:02<00:00, 39070.50 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   2%|▏         | 1451/74005 [00:01<01:04, 1130.33 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  26%|██▌       | 19355/74005 [00:01<00:02, 24369.65 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  51%|█████     | 37670/74005 [00:01<00:00, 43908.95 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  71%|███████   | 52632/74005 [00:02<00:00, 53912.39 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  33%|███▎      | 24612/74005 [00:01<00:01, 29174.97 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  39%|███▉      | 28965/74005 [00:01<00:01, 37986.18 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  81%|████████  | 59985/74005 [00:02<00:00, 57767.82 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  70%|██████▉   | 51632/74005 [00:02<00:00, 61700.95 examples/s]Running bart tokenizers on validationdataset (num_proc=51):   3%|▎         | 2451/74005 [00:01<00:35, 2038.70 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  44%|████▎     | 32318/74005 [00:01<00:01, 40063.83 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  56%|█████▌    | 41573/74005 [00:01<00:00, 56909.34 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  94%|█████████▍| 69495/74005 [00:02<00:00, 66575.69 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  21%|██        | 15451/74005 [00:01<00:02, 19807.01 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  69%|██████▉   | 50926/74005 [00:02<00:00, 75825.05 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  85%|████████▍ | 62691/74005 [00:02<00:00, 69180.45 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  82%|████████▏ | 60377/74005 [00:02<00:00, 45184.39 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  82%|████████▏ | 60377/74005 [00:02<00:00, 34362.31 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  32%|███▏      | 23903/74005 [00:03<00:07, 6637.19 examples/s] Running bart tokenizers on validationdataset (num_proc=51):  96%|█████████▌| 70848/74005 [00:04<00:00, 11639.75 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  91%|█████████▏| 67593/74005 [00:04<00:00, 12275.85 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  91%|█████████ | 67142/74005 [00:04<00:00, 14138.09 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  69%|██████▉   | 51377/74005 [00:04<00:01, 19634.19 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  99%|█████████▉| 73554/74005 [00:04<00:00, 17002.34 examples/s]Running bart tokenizers on validationdataset (num_proc=51): 100%|██████████| 74005/74005 [00:04<00:00, 15094.32 examples/s]Running bart tokenizers on validationdataset (num_proc=51):  93%|█████████▎| 69044/74005 [00:04<00:00, 29728.83 examples/s]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             wandb: Currently logged in as: hou-charlie2 (fanti-lab). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /home/ubuntu/seq2seq_repo/seq2seq/wandb/run-20230622_005710-npdumpvs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-haze-100
wandb: ⭐️ View project at https://wandb.ai/fanti-lab/clm_no_trainer
wandb: 🚀 View run at https://wandb.ai/fanti-lab/clm_no_trainer/runs/npdumpvs
06/22/2023 00:57:10 - INFO - __main__ - ***** Running training *****
06/22/2023 00:57:10 - INFO - __main__ -   Num examples = 666037
06/22/2023 00:57:10 - INFO - __main__ -   Num Epochs = 20
06/22/2023 00:57:10 - INFO - __main__ -   Instantaneous batch size per device = 32
06/22/2023 00:57:10 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 2048
06/22/2023 00:57:10 - INFO - __main__ -   Gradient Accumulation steps = 8
06/22/2023 00:57:10 - INFO - __main__ -   Total optimization steps = 6520
Epoch 0 step 0 loss 3.9321441650390625 percent done 0.0
Epoch 0 step 100 loss 3.195234775543213 percent done 0.03680981595092025
Epoch 0 step 200 loss 3.0163183212280273 percent done 0.07668711656441718
Epoch 0 step 300 loss 2.9587509632110596 percent done 0.11349693251533742
Epoch 0 step 400 loss 2.657787799835205 percent done 0.15337423312883436
Epoch 0 step 500 loss 2.18184494972229 percent done 0.1901840490797546
Epoch 0 step 600 loss 1.8611435890197754 percent done 0.23006134969325154
Epoch 0 step 700 loss 2.222313642501831 percent done 0.2668711656441718
Epoch 0 step 800 loss 2.219209671020508 percent done 0.3067484662576687
Epoch 0 step 900 loss 2.1559600830078125 percent done 0.34355828220858897
Epoch 0 step 1000 loss 2.083414077758789 percent done 0.3834355828220859
Epoch 0 step 1100 loss 2.1690070629119873 percent done 0.42024539877300615
Epoch 0 step 1200 loss 1.983858585357666 percent done 0.4601226993865031
Epoch 0 step 1300 loss 2.022280693054199 percent done 0.49693251533742333
Epoch 0 step 1400 loss 2.0840911865234375 percent done 0.5368098159509203
Epoch 0 step 1500 loss 2.339830160140991 percent done 0.5736196319018405
Epoch 0 step 1600 loss 1.9565298557281494 percent done 0.6134969325153374
Epoch 0 step 1700 loss 2.0490808486938477 percent done 0.6503067484662577
Epoch 0 step 1800 loss 1.746058702468872 percent done 0.6901840490797546
Epoch 0 step 1900 loss 1.8405449390411377 percent done 0.7269938650306749
Epoch 0 step 2000 loss 1.9136552810668945 percent done 0.7668711656441718
Epoch 0 step 2100 loss 1.9875162839889526 percent done 0.803680981595092
Epoch 0 step 2200 loss 1.9442574977874756 percent done 0.843558282208589
Epoch 0 step 2300 loss 1.7483292818069458 percent done 0.8803680981595092
Epoch 0 step 2400 loss 1.786858320236206 percent done 0.9202453987730062
Epoch 0 step 2500 loss 2.114307403564453 percent done 0.9570552147239264
Epoch 0 step 2600 loss 2.023620843887329 percent done 0.9969325153374233
06/22/2023 01:08:23 - INFO - __main__ - epoch 0: perplexity: 5.685698691328693 eval_loss: 1.737954020500183
06/22/2023 01:08:23 - INFO - accelerate.accelerator - Saving current state to /home/ubuntu/bartone/noise0.1/epoch_0
06/22/2023 01:08:24 - INFO - accelerate.checkpointing - Model weights saved in /home/ubuntu/bartone/noise0.1/epoch_0/pytorch_model.bin
06/22/2023 01:08:25 - INFO - accelerate.checkpointing - Optimizer state saved in /home/ubuntu/bartone/noise0.1/epoch_0/optimizer.bin
06/22/2023 01:08:25 - INFO - accelerate.checkpointing - Scheduler state saved in /home/ubuntu/bartone/noise0.1/epoch_0/scheduler.bin
06/22/2023 01:08:25 - INFO - accelerate.checkpointing - Random states saved in /home/ubuntu/bartone/noise0.1/epoch_0/random_states_0.pkl
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

  Id  Text before [test]                                                                                                                                                                    Text after [test]
----  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0  the superior look faded from my face .                                                                                                                                                the superior look faded from my face.
   1  this is hopeless , thought beifar frustrated .                                                                                                                                        this is hopeless, thought beifar frustrated.
   2  'you 've forgotten to fill in the comment card ... ' 85 azif was a tall man , well over six feet tall , but the entity that now confronted the moor was almost half his size again .  'you've forgotten to fill in the comment card...'azmodea was a tall man, well over six feet tall, but the entity that now confronted the moor was almost half his size again.
   3  he looked closer as the wheels crunched through the snow and came to a stop , and saw that the headlights belonged to a tan toyota land cruiser .                                     he looked closer as the wheels crunched through the snow and came to a stop, and saw that the headlights belonged to a tan toyota land cruiser.
   4  the air had become icy over the course of a single day , the sheer dip in temperature was almost paralyzing .                                                                         the air had become icy over the course of a single day, the sheer dip in temperature was almost paralyzing.
   5  chaeli screamed , sliding from her saddle .                                                                                                                                           chaeli screamed, sliding from her saddle.
   6  he was an artist , and michelle modeled for his sketches by day , and sometimes worked on her own fashion designs .                                                                   he was an artist, and michelle modeled for his sketches by day, and sometimes worked on her own fashion designs.
   7  at least her mom would know that she had been here .                                                                                                                                  at least her mom would know that she had been here.
   8  goga had the advantage , but he was losing it .                                                                                                                                       goga had the advantage, but he was losing it.
   9  none have been able to defeat both of the demons at the same time .                                                                                                                   none have been able to defeat both of the demons at the same time.
  10  my father remains silent for another moment .                                                                                                                                         my father remains silent for another moment.
  11  why , in fact , was my head bandage and throbbing like an egg about to hatch ?                                                                                                        why, in fact, was my head bandage and throbbing like an egg about to hatch?
  12  lilly laughed .                                                                                                                                                                       lilly laughed.
  13  took away my strength .                                                                                                                                                               took away my strength.
  14  cordelia opened her mouth to say something , but was stopped by a familiar voice .                                                                                                    cordelia opened her mouth to say something, but was stopped by a familiar voice.
  15  big grin .                                                                                                                                                                            big grin.
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

  Id  Text before [train]                                                                                                                                                         Text after [train]
----  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------
   0  named after your hair ?                                                                                                                                                     named after your hair?
   1  it was coming from the disco in the next room .                                                                                                                             it was coming from the disco in the next room.
   2  marcus blocked every blow with a graceful ease , but still chase attacked savagely .                                                                                        marcus blocked every blow with a graceful ease, but still chase attacked savagely.
   3  and theres nothing to worry , dont fret .                                                                                                                                   and theres nothing to worry, dont fret.
   4  meaning ?                                                                                                                                                                   meaning?
   5  tania did n't want to say it , but she was actually scared of the spear strike .                                                                                            tania didn't want to say it, but she was actually scared of the spear strike.
   6  the posts were carved into the shapes of menwhat looked like warriors , holding swords .                                                                                    the posts were carved into the shapes of menwhat looked like warriors, holding swords.
   7  his voice was firm and deeply set ; his words were chosen well ; his song was filled up by the soul of what he must have felt .                                             his voice was firm and deeply set ; his words were chosen well ; his song was filled up by the soul of what he must have felt.
   8  why didnt you tell me ?                                                                                                                                                     why didnt you tell me?
   9  she let ivans hand go and went to a sink to wash the cream off her own .                                                                                                    she let ivans hand go and went to a sink to wash the cream off her own.
  10  jair then handed nathan a large , rounded clay bowl , full of rich stew with meat and herbs , which smelled more wonderful than anything he had consumed in a long while .  jess then handed nathan a large, rich stew, full bowl of meat with herbs and wonderful stew, which smelled more than anything he had consumed in a long while.
  11  then again , salena was quick to attack the head .                                                                                                                          then again, salena was quick to attack the head.
  12  hes come back for nikki .                                                                                                                                                   hes come back for nikki.
  13  i bounced up the steps so hard that i nearly fell off .                                                                                                                     i bounced up the steps so hard that i nearly fell off.
  14  our immune system is strong so its unlikely for us to develop feverish illness .                                                                                            our immune system is strong so its unlikely for us to develop feverish illness.
  15  before their confrontation could unfold , another dead god interrupted the exchange , asking , what about the mageaous , donacora ?                                         before their confrontation could unfold, another dead god interrupted the exchange, asking, what about the mageaca, donora?
Configuration saved in /home/ubuntu/bartone/noise0.1/epoch_0/config.json
Configuration saved in /home/ubuntu/bartone/noise0.1/epoch_0/generation_config.json
Model weights saved in /home/ubuntu/bartone/noise0.1/epoch_0/pytorch_model.bin
tokenizer config file saved in /home/ubuntu/bartone/noise0.1/epoch_0/bart_tokenizer/tokenizer_config.json
Special tokens file saved in /home/ubuntu/bartone/noise0.1/epoch_0/bart_tokenizer/special_tokens_map.json
Epoch 1 step 0 loss 1.8866369724273682 percent done 1.0
Epoch 1 step 100 loss 1.9714417457580566 percent done 1.0368098159509203
Epoch 1 step 200 loss 1.8959839344024658 percent done 1.0766871165644172
Epoch 1 step 300 loss 2.115030288696289 percent done 1.1134969325153374
Epoch 1 step 400 loss 2.1897988319396973 percent done 1.1533742331288344
Epoch 1 step 500 loss 2.0337414741516113 percent done 1.1901840490797546
Epoch 1 step 600 loss 1.6343247890472412 percent done 1.2300613496932515
Epoch 1 step 700 loss 1.8400840759277344 percent done 1.2668711656441718
Epoch 1 step 800 loss 1.589642882347107 percent done 1.3067484662576687
Epoch 1 step 900 loss 1.8382434844970703 percent done 1.343558282208589
Epoch 1 step 1000 loss 1.8211296796798706 percent done 1.383435582822086
Epoch 1 step 1100 loss 2.038801670074463 percent done 1.4202453987730062
Epoch 1 step 1200 loss 1.8381781578063965 percent done 1.460122699386503
Epoch 1 step 1300 loss 1.599007248878479 percent done 1.4969325153374233
Epoch 1 step 1400 loss 1.864454984664917 percent done 1.5368098159509203
Epoch 1 step 1500 loss 1.7691410779953003 percent done 1.5736196319018405
Epoch 1 step 1600 loss 1.8006160259246826 percent done 1.6134969325153374
Epoch 1 step 1700 loss 1.6798861026763916 percent done 1.6503067484662577
Epoch 1 step 1800 loss 1.6349432468414307 percent done 1.6901840490797546
Epoch 1 step 1900 loss 1.887007236480713 percent done 1.7269938650306749
Epoch 1 step 2000 loss 1.7877147197723389 percent done 1.7668711656441718
Epoch 1 step 2100 loss 1.7448089122772217 percent done 1.803680981595092
Epoch 1 step 2200 loss 1.9266754388809204 percent done 1.843558282208589
Epoch 1 step 2300 loss 1.6394636631011963 percent done 1.8803680981595092
Epoch 1 step 2400 loss 1.7823245525360107 percent done 1.9202453987730062
Epoch 1 step 2500 loss 2.1075985431671143 percent done 1.9570552147239264
Epoch 1 step 2600 loss 1.707303524017334 percent done 1.9969325153374233
06/22/2023 01:19:51 - INFO - __main__ - epoch 1: perplexity: 5.296711962623902 eval_loss: 1.6670862436294556
06/22/2023 01:19:51 - INFO - accelerate.accelerator - Saving current state to /home/ubuntu/bartone/noise0.1/epoch_1
06/22/2023 01:19:52 - INFO - accelerate.checkpointing - Model weights saved in /home/ubuntu/bartone/noise0.1/epoch_1/pytorch_model.bin
06/22/2023 01:19:53 - INFO - accelerate.checkpointing - Optimizer state saved in /home/ubuntu/bartone/noise0.1/epoch_1/optimizer.bin
06/22/2023 01:19:53 - INFO - accelerate.checkpointing - Scheduler state saved in /home/ubuntu/bartone/noise0.1/epoch_1/scheduler.bin
06/22/2023 01:19:53 - INFO - accelerate.checkpointing - Random states saved in /home/ubuntu/bartone/noise0.1/epoch_1/random_states_0.pkl
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

  Id  Text before [test]                                                                                                                                                                    Text after [test]
----  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0  the superior look faded from my face .                                                                                                                                                the superior look faded from my face.
   1  this is hopeless , thought beifar frustrated .                                                                                                                                        this is hopeless, thought beifar frustrated.
   2  'you 've forgotten to fill in the comment card ... ' 85 azif was a tall man , well over six feet tall , but the entity that now confronted the moor was almost half his size again .  'you've forgotten to fill in the comment card...'azif was a tall man, well over six feet tall, but the entity that now confronted the moor was almost half his size again.
   3  he looked closer as the wheels crunched through the snow and came to a stop , and saw that the headlights belonged to a tan toyota land cruiser .                                     he looked closer as the wheels crunched through the snow and came to a stop, and saw that the headlights belonged to a tan toyota land.
   4  the air had become icy over the course of a single day , the sheer dip in temperature was almost paralyzing .                                                                         the air had become icy over the course of a single day, the sheer dip in temperature was almost paralyzing.
   5  chaeli screamed , sliding from her saddle .                                                                                                                                           chaeli screamed, sliding from her saddle.
   6  he was an artist , and michelle modeled for his sketches by day , and sometimes worked on her own fashion designs .                                                                   he was an artist, and michelle modeled for his sketches by day, and sometimes worked on her own fashion designs.
   7  at least her mom would know that she had been here .                                                                                                                                  at least her mom would know that she had been here.
   8  goga had the advantage , but he was losing it .                                                                                                                                       goga had the advantage, but he was losing it.
   9  none have been able to defeat both of the demons at the same time .                                                                                                                   none have been able to defeat both of the demons at the same time.
  10  my father remains silent for another moment .                                                                                                                                         my father remains silent for another moment.
  11  why , in fact , was my head bandage and throbbing like an egg about to hatch ?                                                                                                        why, in fact, was my head bandage and throbbing like an egg about to hatch?
  12  lilly laughed .                                                                                                                                                                       lilly laughed.
  13  took away my strength .                                                                                                                                                               took away my strength.
  14  cordelia opened her mouth to say something , but was stopped by a familiar voice .                                                                                                    cordelia opened her mouth to say something, but was stopped by a familiar voice.
  15  big grin .                                                                                                                                                                            big grin.
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

  Id  Text before [train]                                                                                                                                                         Text after [train]
----  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------
   0  named after your hair ?                                                                                                                                                     named after your hair?
   1  it was coming from the disco in the next room .                                                                                                                             it was coming from the disco in the next room.
   2  marcus blocked every blow with a graceful ease , but still chase attacked savagely .                                                                                        marcus blocked every blow with a graceful ease, but still chase attacked savagely.
   3  and theres nothing to worry , dont fret .                                                                                                                                   and theres nothing to worry, dont fret.
   4  meaning ?                                                                                                                                                                   meaning?
   5  tania did n't want to say it , but she was actually scared of the spear strike .                                                                                            tania didn't want to say it, but she was actually scared of the spear strike.
   6  the posts were carved into the shapes of menwhat looked like warriors , holding swords .                                                                                    the posts were carved into the shapes of menwhat looked like warriors, holding swords.
   7  his voice was firm and deeply set ; his words were chosen well ; his song was filled up by the soul of what he must have felt .                                             his voice was firm and deeply set ; his words were chosen well ; his song was filled up by the soul of what he must have felt.
   8  why didnt you tell me ?                                                                                                                                                     why didnt you tell me?
   9  she let ivans hand go and went to a sink to wash the cream off her own .                                                                                                    she let ivans hand go and went to a sink to wash the cream off her own.
  10  jair then handed nathan a large , rounded clay bowl , full of rich stew with meat and herbs , which smelled more wonderful than anything he had consumed in a long while .  jair then handed nathan a large, rich stew, full bowl of meat with herbs and wonderful stew, which smelled more than anything he had consumed in a long while.
  11  then again , salena was quick to attack the head .                                                                                                                          then again, salena was quick to attack the head.
  12  hes come back for nikki .                                                                                                                                                   hes come back for nikki.
  13  i bounced up the steps so hard that i nearly fell off .                                                                                                                     i bounced up the steps so hard that i nearly fell off.
  14  our immune system is strong so its unlikely for us to develop feverish illness .                                                                                            our immune system is strong so its unlikely for us to develop feverish illness.
  15  before their confrontation could unfold , another dead god interrupted the exchange , asking , what about the mageaous , donacora ?                                         before their confrontation could unfold, another dead god interrupted the exchange, asking about what, donaacora?
Configuration saved in /home/ubuntu/bartone/noise0.1/epoch_1/config.json
Configuration saved in /home/ubuntu/bartone/noise0.1/epoch_1/generation_config.json
Model weights saved in /home/ubuntu/bartone/noise0.1/epoch_1/pytorch_model.bin
tokenizer config file saved in /home/ubuntu/bartone/noise0.1/epoch_1/bart_tokenizer/tokenizer_config.json
Special tokens file saved in /home/ubuntu/bartone/noise0.1/epoch_1/bart_tokenizer/special_tokens_map.json
Epoch 2 step 0 loss 1.7601503133773804 percent done 2.0
Epoch 2 step 100 loss 1.8834080696105957 percent done 2.03680981595092
Epoch 2 step 200 loss 1.699023723602295 percent done 2.076687116564417
Epoch 2 step 300 loss 1.73410964012146 percent done 2.1134969325153374
Epoch 2 step 400 loss 1.6465389728546143 percent done 2.1533742331288344
Epoch 2 step 500 loss 1.6542716026306152 percent done 2.1901840490797544
Epoch 2 step 600 loss 1.8426406383514404 percent done 2.2300613496932513
Epoch 2 step 700 loss 1.7016916275024414 percent done 2.266871165644172
Epoch 2 step 800 loss 1.8168294429779053 percent done 2.3067484662576687
Epoch 2 step 900 loss 1.859641432762146 percent done 2.3435582822085887
Epoch 2 step 1000 loss 1.808005690574646 percent done 2.3834355828220857
Epoch 2 step 1100 loss 1.6772170066833496 percent done 2.420245398773006
Epoch 2 step 1200 loss 1.6149356365203857 percent done 2.460122699386503
Epoch 2 step 1300 loss 1.7556955814361572 percent done 2.496932515337423
Epoch 2 step 1400 loss 1.5968890190124512 percent done 2.53680981595092
Epoch 2 step 1500 loss 1.827525019645691 percent done 2.5736196319018405
Epoch 2 step 1600 loss 1.6442151069641113 percent done 2.6134969325153374
Epoch 2 step 1700 loss 1.7410341501235962 percent done 2.6503067484662575
Epoch 2 step 1800 loss 1.766262173652649 percent done 2.6901840490797544
Epoch 2 step 1900 loss 1.9012128114700317 percent done 2.726993865030675
Epoch 2 step 2000 loss 1.7390918731689453 percent done 2.766871165644172
Epoch 2 step 2100 loss 1.694097876548767 percent done 2.803680981595092
Epoch 2 step 2200 loss 1.813913345336914 percent done 2.8435582822085887
Epoch 2 step 2300 loss 1.7493809461593628 percent done 2.8803680981595092
Epoch 2 step 2400 loss 1.7384371757507324 percent done 2.920245398773006
Epoch 2 step 2500 loss 1.8814976215362549 percent done 2.957055214723926
Epoch 2 step 2600 loss 1.8814029693603516 percent done 2.996932515337423
06/22/2023 01:31:20 - INFO - __main__ - epoch 2: perplexity: 5.137082638175783 eval_loss: 1.6364853382110596
06/22/2023 01:31:20 - INFO - accelerate.accelerator - Saving current state to /home/ubuntu/bartone/noise0.1/epoch_2
06/22/2023 01:31:21 - INFO - accelerate.checkpointing - Model weights saved in /home/ubuntu/bartone/noise0.1/epoch_2/pytorch_model.bin
06/22/2023 01:31:23 - INFO - accelerate.checkpointing - Optimizer state saved in /home/ubuntu/bartone/noise0.1/epoch_2/optimizer.bin
06/22/2023 01:31:23 - INFO - accelerate.checkpointing - Scheduler state saved in /home/ubuntu/bartone/noise0.1/epoch_2/scheduler.bin
06/22/2023 01:31:23 - INFO - accelerate.checkpointing - Random states saved in /home/ubuntu/bartone/noise0.1/epoch_2/random_states_0.pkl
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

  Id  Text before [test]                                                                                                                                                                    Text after [test]
----  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0  the superior look faded from my face .                                                                                                                                                the superior look faded from my face.
   1  this is hopeless , thought beifar frustrated .                                                                                                                                        this is hopeless, thought beifar frustrated.
   2  'you 've forgotten to fill in the comment card ... ' 85 azif was a tall man , well over six feet tall , but the entity that now confronted the moor was almost half his size again .  'you've forgotten to fill in the comment card...'azif was a tall man, well over six feet tall, but the entity that now confronted the moor was almost half his size again.
   3  he looked closer as the wheels crunched through the snow and came to a stop , and saw that the headlights belonged to a tan toyota land cruiser .                                     he looked closer as the wheels crunched through the snow and came to a stop, and saw that the headlights belonged to a tan toyota land.
   4  the air had become icy over the course of a single day , the sheer dip in temperature was almost paralyzing .                                                                         the air had become icy over the course of a single day, the sheer dip in temperature was almost paralyzing.
   5  chaeli screamed , sliding from her saddle .                                                                                                                                           chaeli screamed, sliding from her saddle.
   6  he was an artist , and michelle modeled for his sketches by day , and sometimes worked on her own fashion designs .                                                                   he was an artist, and michelle modeled for his sketches by day, and sometimes worked on her own fashion designs.
   7  at least her mom would know that she had been here .                                                                                                                                  at least her mom would know that she had been here.
   8  goga had the advantage , but he was losing it .                                                                                                                                       goga had the advantage, but he was losing it.
   9  none have been able to defeat both of the demons at the same time .                                                                                                                   none have been able to defeat both of the demons at the same time.
  10  my father remains silent for another moment .                                                                                                                                         my father remains silent for another moment.
  11  why , in fact , was my head bandage and throbbing like an egg about to hatch ?                                                                                                        why, in fact, was my head bandage and throbbing like an egg about to hatch?
  12  lilly laughed .                                                                                                                                                                       lilly laughed.
  13  took away my strength .                                                                                                                                                               took away my strength.
  14  cordelia opened her mouth to say something , but was stopped by a familiar voice .                                                                                                    cordelia opened her mouth to say something, but was stopped by a familiar voice.
  15  big grin .                                                                                                                                                                            big grin.
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

  Id  Text before [train]                                                                                                                                                         Text after [train]
----  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0  named after your hair ?                                                                                                                                                     named after your hair?
   1  it was coming from the disco in the next room .                                                                                                                             it was coming from the disco in the next room.
   2  marcus blocked every blow with a graceful ease , but still chase attacked savagely .                                                                                        marcus blocked every blow with a graceful ease, but still chase attacked savagely.
   3  and theres nothing to worry , dont fret .                                                                                                                                   and theres nothing to worry, dont fret.
   4  meaning ?                                                                                                                                                                   meaning?
   5  tania did n't want to say it , but she was actually scared of the spear strike .                                                                                            tania didn't want to say it, but she was actually scared of the spear strike.
   6  the posts were carved into the shapes of menwhat looked like warriors , holding swords .                                                                                    the posts were carved into the shapes of menwhat looked like warriors, holding swords.
   7  his voice was firm and deeply set ; his words were chosen well ; his song was filled up by the soul of what he must have felt .                                             his voice was firm and deeply set ; his words were chosen well ; his song was filled up by the soul of what he must have felt.
   8  why didnt you tell me ?                                                                                                                                                     why didnt you tell me?
   9  she let ivans hand go and went to a sink to wash the cream off her own .                                                                                                    she let ivans hand go and went to a sink to wash the cream off her own.
  10  jair then handed nathan a large , rounded clay bowl , full of rich stew with meat and herbs , which smelled more wonderful than anything he had consumed in a long while .  jair then handed nathan a large, rounded stew, full of rich meat with herbs and stew, which smelled more wonderful than anything he had consumed in a long while.
  11  then again , salena was quick to attack the head .                                                                                                                          then again, salena was quick to attack the head.
  12  hes come back for nikki .                                                                                                                                                   hes come back for nikki.
  13  i bounced up the steps so hard that i nearly fell off .                                                                                                                     i bounced up the steps so hard that i nearly fell off.
  14  our immune system is strong so its unlikely for us to develop feverish illness .                                                                                            our immune system is strong so its unlikely for us to develop feverish illness.
  15  before their confrontation could unfold , another dead god interrupted the exchange , asking , what about the mageaous , donacora ?                                         before their confrontation could unfold, another dead god interrupted the exchange, asking about the mage, whataacous?
Configuration saved in /home/ubuntu/bartone/noise0.1/epoch_2/config.json
Configuration saved in /home/ubuntu/bartone/noise0.1/epoch_2/generation_config.json
Model weights saved in /home/ubuntu/bartone/noise0.1/epoch_2/pytorch_model.bin
tokenizer config file saved in /home/ubuntu/bartone/noise0.1/epoch_2/bart_tokenizer/tokenizer_config.json
Special tokens file saved in /home/ubuntu/bartone/noise0.1/epoch_2/bart_tokenizer/special_tokens_map.json
Epoch 3 step 0 loss 1.6933319568634033 percent done 3.0
Epoch 3 step 100 loss 1.693630576133728 percent done 3.03680981595092
Epoch 3 step 200 loss 1.6780844926834106 percent done 3.076687116564417
Epoch 3 step 300 loss 1.787458896636963 percent done 3.1134969325153374
Epoch 3 step 400 loss 1.886004090309143 percent done 3.1533742331288344
Epoch 3 step 500 loss 1.7551703453063965 percent done 3.1901840490797544
Epoch 3 step 600 loss 1.6383516788482666 percent done 3.2300613496932513
Epoch 3 step 700 loss 1.6556404829025269 percent done 3.266871165644172
Epoch 3 step 800 loss 1.8449268341064453 percent done 3.3067484662576687
Epoch 3 step 900 loss 1.6131196022033691 percent done 3.3435582822085887
Epoch 3 step 1000 loss 1.689062237739563 percent done 3.3834355828220857
Epoch 3 step 1100 loss 1.7476526498794556 percent done 3.420245398773006
Epoch 3 step 1200 loss 1.7289260625839233 percent done 3.460122699386503
Epoch 3 step 1300 loss 1.7378290891647339 percent done 3.496932515337423
Epoch 3 step 1400 loss 1.666473388671875 percent done 3.53680981595092
Epoch 3 step 1500 loss 1.5871610641479492 percent done 3.5736196319018405
Epoch 3 step 1600 loss 1.60613214969635 percent done 3.6134969325153374
Epoch 3 step 1700 loss 1.7161463499069214 percent done 3.6503067484662575
Epoch 3 step 1800 loss 1.6314488649368286 percent done 3.6901840490797544
Epoch 3 step 1900 loss 2.05718994140625 percent done 3.726993865030675
Epoch 3 step 2000 loss 1.6825051307678223 percent done 3.766871165644172
Epoch 3 step 2100 loss 1.7971606254577637 percent done 3.803680981595092
Epoch 3 step 2200 loss 1.7395786046981812 percent done 3.8435582822085887
Epoch 3 step 2300 loss 1.8804724216461182 percent done 3.8803680981595092
Epoch 3 step 2400 loss 1.873887538909912 percent done 3.920245398773006
Epoch 3 step 2500 loss 1.6709339618682861 percent done 3.957055214723926
Epoch 3 step 2600 loss 1.6752575635910034 percent done 3.996932515337423
06/22/2023 01:42:47 - INFO - __main__ - epoch 3: perplexity: 5.032352268931975 eval_loss: 1.6158875226974487
06/22/2023 01:42:47 - INFO - accelerate.accelerator - Saving current state to /home/ubuntu/bartone/noise0.1/epoch_3
06/22/2023 01:42:48 - INFO - accelerate.checkpointing - Model weights saved in /home/ubuntu/bartone/noise0.1/epoch_3/pytorch_model.bin
06/22/2023 01:43:11 - INFO - accelerate.checkpointing - Optimizer state saved in /home/ubuntu/bartone/noise0.1/epoch_3/optimizer.bin
06/22/2023 01:43:11 - INFO - accelerate.checkpointing - Scheduler state saved in /home/ubuntu/bartone/noise0.1/epoch_3/scheduler.bin
06/22/2023 01:43:11 - INFO - accelerate.checkpointing - Random states saved in /home/ubuntu/bartone/noise0.1/epoch_3/random_states_0.pkl
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

  Id  Text before [test]                                                                                                                                                                    Text after [test]
----  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0  the superior look faded from my face .                                                                                                                                                the superior look faded from my face.
   1  this is hopeless , thought beifar frustrated .                                                                                                                                        this is hopeless, thought beifar frustrated.
   2  'you 've forgotten to fill in the comment card ... ' 85 azif was a tall man , well over six feet tall , but the entity that now confronted the moor was almost half his size again .  'you've forgotten to fill in the comment card...'azif was a tall man, well over six feet tall, but the entity that now confronted the moor was almost half his size again.
   3  he looked closer as the wheels crunched through the snow and came to a stop , and saw that the headlights belonged to a tan toyota land cruiser .                                     he looked closer as the wheels crunched through the snow and came to a stop, and saw that the headlights belonged to a tan toyota land cruiser.
   4  the air had become icy over the course of a single day , the sheer dip in temperature was almost paralyzing .                                                                         the air had become icy over the course of a single day, the sheer dip in temperature was almost paralyzing.
   5  chaeli screamed , sliding from her saddle .                                                                                                                                           chaeli screamed, sliding from her saddle.
   6  he was an artist , and michelle modeled for his sketches by day , and sometimes worked on her own fashion designs .                                                                   he was an artist, and michelle modeled for his sketches by day, and sometimes worked on her own fashion designs.
   7  at least her mom would know that she had been here .                                                                                                                                  at least her mom would know that she had been here.
   8  goga had the advantage , but he was losing it .                                                                                                                                       goga had the advantage, but he was losing it.
   9  none have been able to defeat both of the demons at the same time .                                                                                                                   none have been able to defeat both of the demons at the same time.
  10  my father remains silent for another moment .                                                                                                                                         my father remains silent for another moment.
  11  why , in fact , was my head bandage and throbbing like an egg about to hatch ?                                                                                                        why, in fact, was my head bandage and throbbing like an egg about to hatch?
  12  lilly laughed .                                                                                                                                                                       lilly laughed.
  13  took away my strength .                                                                                                                                                               took away my strength.
  14  cordelia opened her mouth to say something , but was stopped by a familiar voice .                                                                                                    cordelia opened her mouth to say something, but was stopped by a familiar voice.
  15  big grin .                                                                                                                                                                            big grin.
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

  Id  Text before [train]                                                                                                                                                         Text after [train]
----  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0  named after your hair ?                                                                                                                                                     named after your hair?
   1  it was coming from the disco in the next room .                                                                                                                             it was coming from the disco in the next room.
   2  marcus blocked every blow with a graceful ease , but still chase attacked savagely .                                                                                        marcus blocked every blow with a graceful ease, but still chase attacked savagely.
   3  and theres nothing to worry , dont fret .                                                                                                                                   and theres nothing to worry, dont fret.
   4  meaning ?                                                                                                                                                                   meaning?
   5  tania did n't want to say it , but she was actually scared of the spear strike .                                                                                            tania didn't want to say it, but she was actually scared of the spear strike.
   6  the posts were carved into the shapes of menwhat looked like warriors , holding swords .                                                                                    the posts were carved into the shapes of menwhat looked like warriors, holding swords.
   7  his voice was firm and deeply set ; his words were chosen well ; his song was filled up by the soul of what he must have felt .                                             his voice was firm and deeply set ; his words were chosen well ; his song was filled up by the soul of what he must have felt.
   8  why didnt you tell me ?                                                                                                                                                     why didnt you tell me?
   9  she let ivans hand go and went to a sink to wash the cream off her own .                                                                                                    she let ivans hand go and went to a sink to wash the cream off her own.
  10  jair then handed nathan a large , rounded clay bowl , full of rich stew with meat and herbs , which smelled more wonderful than anything he had consumed in a long while .  jair then handed nathan a large, rounded stew, full of rich meat with herbs and stew, which smelled more wonderful than anything he had consumed in a long while.
  11  then again , salena was quick to attack the head .                                                                                                                          then again, salena was quick to attack the head.
  12  hes come back for nikki .                                                                                                                                                   hes come back for nikki.
  13  i bounced up the steps so hard that i nearly fell off .                                                                                                                     i bounced up the steps so hard that i nearly fell off.
  14  our immune system is strong so its unlikely for us to develop feverish illness .                                                                                            our immune system is strong so its unlikely for us to develop feverish illness.
  15  before their confrontation could unfold , another dead god interrupted the exchange , asking , what about the mageaous , donacora ?                                         before their confrontation could unfold, another dead god interrupted the exchange, asking, what about the mageousa, donacora?
Configuration saved in /home/ubuntu/bartone/noise0.1/epoch_3/config.json
Configuration saved in /home/ubuntu/bartone/noise0.1/epoch_3/generation_config.json
Model weights saved in /home/ubuntu/bartone/noise0.1/epoch_3/pytorch_model.bin
tokenizer config file saved in /home/ubuntu/bartone/noise0.1/epoch_3/bart_tokenizer/tokenizer_config.json
Special tokens file saved in /home/ubuntu/bartone/noise0.1/epoch_3/bart_tokenizer/special_tokens_map.json
Epoch 4 step 0 loss 1.6246120929718018 percent done 4.0
Epoch 4 step 100 loss 1.9103386402130127 percent done 4.03680981595092
Epoch 4 step 200 loss 1.7822885513305664 percent done 4.076687116564417
Epoch 4 step 300 loss 1.7291208505630493 percent done 4.113496932515337
Epoch 4 step 400 loss 1.8094780445098877 percent done 4.153374233128834
Epoch 4 step 500 loss 1.674628734588623 percent done 4.190184049079755
Epoch 4 step 600 loss 1.6815192699432373 percent done 4.230061349693251
Epoch 4 step 700 loss 1.735891342163086 percent done 4.266871165644171
Epoch 4 step 800 loss 1.6158791780471802 percent done 4.306748466257669
Epoch 4 step 900 loss 1.675247073173523 percent done 4.343558282208589
Epoch 4 step 1000 loss 1.6108487844467163 percent done 4.383435582822086
Epoch 4 step 1100 loss 1.577852487564087 percent done 4.420245398773006
Epoch 4 step 1200 loss 1.6642950773239136 percent done 4.460122699386503
Epoch 4 step 1300 loss 1.6659940481185913 percent done 4.4969325153374236
Epoch 4 step 1400 loss 1.6561391353607178 percent done 4.53680981595092
Epoch 4 step 1500 loss 1.6246082782745361 percent done 4.57361963190184
Epoch 4 step 1600 loss 1.6649751663208008 percent done 4.613496932515337
Epoch 4 step 1700 loss 1.709149718284607 percent done 4.6503067484662575
Epoch 4 step 1800 loss 1.5902923345565796 percent done 4.690184049079755
Epoch 4 step 1900 loss 1.5829908847808838 percent done 4.726993865030675
Epoch 4 step 2000 loss 1.567095160484314 percent done 4.766871165644171
Epoch 4 step 2100 loss 1.6014167070388794 percent done 4.803680981595092
Epoch 4 step 2200 loss 1.6608874797821045 percent done 4.843558282208589
Epoch 4 step 2300 loss 1.5912330150604248 percent done 4.880368098159509
Epoch 4 step 2400 loss 1.7062764167785645 percent done 4.920245398773006
Epoch 4 step 2500 loss 1.6491384506225586 percent done 4.957055214723926
Epoch 4 step 2600 loss 1.6511685848236084 percent done 4.9969325153374236
06/22/2023 01:54:43 - INFO - __main__ - epoch 4: perplexity: 4.946021811730984 eval_loss: 1.5985835790634155
06/22/2023 01:54:43 - INFO - accelerate.accelerator - Saving current state to /home/ubuntu/bartone/noise0.1/epoch_4
06/22/2023 01:54:50 - INFO - accelerate.checkpointing - Model weights saved in /home/ubuntu/bartone/noise0.1/epoch_4/pytorch_model.bin
Traceback (most recent call last):
  File "/home/ubuntu/venv/lib/python3.9/site-packages/torch/serialization.py", line 441, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/torch/serialization.py", line 668, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/151: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/seq2seq_repo/seq2seq/run_bart_noise.py", line 266, in <module>
    accelerator.save_state(curr_output_dir)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/accelerator.py", line 2279, in save_state
    save_location = save_accelerator_state(
  File "/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/checkpointing.py", line 81, in save_accelerator_state
    save(state, output_optimizer_file)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/utils/other.py", line 92, in save
    torch.save(obj, f)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/torch/serialization.py", line 442, in save
    return
  File "/home/ubuntu/venv/lib/python3.9/site-packages/torch/serialization.py", line 291, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:337] . unexpected pos 297622080 vs 297621972
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
--- Logging error ---
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/usr/local/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 28] No space left on device
Call stack:
  File "/usr/local/lib/python3.9/threading.py", line 937, in _bootstrap
    self._bootstrap_inner()
  File "/usr/local/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/ubuntu/venv/lib/python3.9/site-packages/wandb/sdk/internal/internal_util.py", line 49, in run
    self._run()
  File "/home/ubuntu/venv/lib/python3.9/site-packages/wandb/sdk/internal/internal_util.py", line 101, in _run
    self._finish()
  File "/home/ubuntu/venv/lib/python3.9/site-packages/wandb/sdk/internal/internal.py", line 331, in _finish
    self._sm.finish()
  File "/home/ubuntu/venv/lib/python3.9/site-packages/wandb/sdk/internal/sender.py", line 1555, in finish
    self._dir_watcher.finish()
  File "/home/ubuntu/venv/lib/python3.9/site-packages/wandb/filesync/dir_watcher.py", line 409, in finish
    logger.info("scan save: %s %s", file_path, save_name)
Message: 'scan save: %s %s'
Arguments: ('/home/ubuntu/seq2seq_repo/seq2seq/wandb/run[E ProcessGroupNCCL.cpp:828] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=30803, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1806587 milliseconds before timing out.
Traceback (most recent call last):
  File "/home/ubuntu/seq2seq_repo/seq2seq/run_bart_noise.py", line 207, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/seq2seq_repo/seq2seq/run_bart_noise.py", line 207, in <module>
    for step, batch in enumerate(train_dataloader):
  File "/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/data_loader.py", line 369, in __iter__
    synchronize_rng_states(self.rng_types, self.synchronized_generator)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/utils/random.py", line 89, in synchronize_rng_states
    for step, batch in enumerate(train_dataloader):
  File "/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/data_loader.py", line 369, in __iter__
    synchronize_rng_states(self.rng_types, self.synchronized_generator)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/utils/random.py", line 89, in synchronize_rng_states
    synchronize_rng_state(RNGType(rng_type), generator=generator)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/utils/random.py", line 84, in synchronize_rng_state
    synchronize_rng_state(RNGType(rng_type), generator=generator)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/accelerate/utils/random.py", line 84, in synchronize_rng_state
    generator.set_state(rng_state)
RuntimeError:     Invalid mt19937 stategenerator.set_state(rng_state)

RuntimeError: Invalid mt19937 state
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 10217 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 10218) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/local/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_bart_noise.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-06-22_02:24:55
  host      : charlie-dev
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 10219)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-06-22_02:24:55
  host      : charlie-dev
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 10221)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-06-22_02:24:55
  host      : charlie-dev
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 10225)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-06-22_02:24:55
  host      : charlie-dev
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 10227)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-06-22_02:24:55
  host      : charlie-dev
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 10228)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2023-06-22_02:24:55
  host      : charlie-dev
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 10229)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-06-22_02:24:55
  host      : charlie-dev
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 10218)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
